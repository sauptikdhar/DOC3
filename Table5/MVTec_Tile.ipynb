{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DOC and DOC3 results for tile data\n",
    "# Table 5 and 6 (correlation) results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import logging\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import mvtec as md   \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    \n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For code optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment for code optimization\n",
    "#torch.backends.cudnn.benchmark = True\n",
    "\n",
    "NUM_WORKERS = int(os.cpu_count() / 2)\n",
    "print('number of workers: ', NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a. Evaluation (Loss, Correlations, Eval Metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function\n",
    "class HingeLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HingeLoss,self).__init__()\n",
    "    \n",
    "    def forward(self, ypred, ytrue, margin = 1.0, smooth = False):\n",
    "        ypred = ypred.squeeze()\n",
    "        if smooth:\n",
    "            loss = torch.nn.Softplus()\n",
    "            out = torch.mean(loss(margin - (ytrue * ypred)))\n",
    "        else:\n",
    "            out = torch.mean(torch.relu(margin - (ytrue * ypred)))\n",
    "        return out\n",
    "\n",
    "\n",
    "def SigmaTh2(trn_dat, univ_dat, net, feat): # Complete Data (not data loaders)\n",
    "    eps = 1e-9\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        trn_dat = trn_dat.to(device).float()\n",
    "        univ_dat = univ_dat.to(device).float()\n",
    "        \n",
    "        Z = net(trn_dat, feat = feat) # 'cnn' and 'final'\n",
    "        U = net(univ_dat, feat = feat)\n",
    "\n",
    "        Z = Z.data.cpu().numpy()\n",
    "        U = U.data.cpu().numpy()\n",
    "        a = np.array([1,-1]).reshape(2,1)\n",
    "        V = np.kron(U,a)\n",
    "        \n",
    "        VZT = np.dot(V,Z.T)\n",
    "        ZVT = np.dot(Z,V.T)\n",
    "        \n",
    "        sig_num = np.trace(np.dot(VZT,ZVT))\n",
    "        ZTZ = np.trace(np.dot(Z.T,Z))\n",
    "        VVT = np.dot(V,V.T)\n",
    "        VVTr = np.trace(VVT) \n",
    "        I_VVT = VVT.shape[0] + VVTr\n",
    "        sig_den = (ZTZ*VVTr)\n",
    "        sig_den2 = (ZTZ*I_VVT)\n",
    "        \n",
    "    return (sig_num/sig_den, sig_num/sig_den2, sig_num, sig_den, sig_den2, ZTZ, VVTr, I_VVT)\n",
    "\n",
    "\n",
    "def evalmetrics(y_true,scores):\n",
    "    auc_score = roc_auc_score(y_true, scores)\n",
    "    \n",
    "    return auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LeNET](LeNET.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10_LeNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CIFAR10_LeNet, self).__init__()\n",
    "\n",
    "        self.rep_dim = 128\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 32, 5, bias=False, padding=2)\n",
    "        self.bn2d1 = nn.BatchNorm2d(32, eps=1e-04, affine=False)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 5, bias=False, padding=2)\n",
    "        self.bn2d2 = nn.BatchNorm2d(64, eps=1e-04, affine=False)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 5, bias=False, padding=2)\n",
    "        self.bn2d3 = nn.BatchNorm2d(128, eps=1e-04, affine=False)\n",
    "        self.fc1 = nn.Linear(128 * 8 * 8, self.rep_dim, bias=False)\n",
    "        self.fc2 = nn.Linear(self.rep_dim, int(self.rep_dim/2), bias=False)\n",
    "        self.fc3 = nn.Linear(int(self.rep_dim/2), 1, bias=False)\n",
    "\n",
    "    def forward(self, x, feat = 'default'):\n",
    "        \n",
    "        if feat in 'none':\n",
    "            return x.view(x.size(0), -1)\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.pool(F.leaky_relu(self.bn2d1(x)))\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool(F.leaky_relu(self.bn2d2(x)))\n",
    "        x = self.conv3(x)\n",
    "        x = self.pool(F.leaky_relu(self.bn2d3(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        if feat in 'cnn':\n",
    "            return x\n",
    "        \n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        \n",
    "        if feat in 'final':\n",
    "            return x\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##############################################################################################\n",
    "### To use our data loader please download all the MVTec data available at:- \n",
    "### https://www.mvtec.com/company/research/datasets/mvtec-ad\n",
    "### And save them in the folder ./mvtec\n",
    "##############################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMSHAPE = 64\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor()])\n",
    "\n",
    "\n",
    "##############################################################################################\n",
    "## To use our data loader please download all the MVTec data available at:- \n",
    "## https://www.mvtec.com/company/research/datasets/mvtec-ad\n",
    "## And save them in the folder ./mvtec\n",
    "##\n",
    "###############################################################################################\n",
    "#                                     Train Data\n",
    "###############################################################################################\n",
    "\n",
    "train_dat = md.MVTEC(root=\"./mvtec\", train=True, transform=transform, resize=IMSHAPE, interpolation=3, category='tile')\n",
    "train_loader = torch.utils.data.DataLoader(train_dat, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True, num_workers=NUM_WORKERS)\n",
    "\n",
    "\n",
    "###############################################################################################\n",
    "#                                     Evaluations on :-\n",
    "###############################################################################################\n",
    "\n",
    "# Test Data\n",
    "test_dat = md.MVTEC(root=\"./mvtec\", train=False, transform=transform,resize=IMSHAPE, interpolation=3, category='tile')\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dat, batch_size=117, shuffle=False, pin_memory=True, num_workers=NUM_WORKERS)\n",
    "\n",
    "# Train (used to compute correlations)\n",
    "train_loader_eval = torch.utils.data.DataLoader(train_dat, batch_size=280, shuffle=False, pin_memory=True, num_workers=NUM_WORKERS)\n",
    "\n",
    "\n",
    "# Objects Eval\n",
    "objs = ['bottle', 'cable', 'capsule', 'hazelnut', 'metal_nut', 'pill', 'transistor']\n",
    "ntrn = [209,224,219,391,220,267,213]\n",
    "ntst = [83,150,132,110,115,167,100]\n",
    "\n",
    "obj_data_loader_eval = list()\n",
    "obj_data_tst_loader_eval = list()\n",
    "\n",
    "for (i,obj) in enumerate(objs):\n",
    "    obj_dat = md.MVTEC(root=\"./mvtec\", train=True, transform=transform, resize=IMSHAPE, interpolation=3, category=obj)\n",
    "    obj_data_loader_eval.append(torch.utils.data.DataLoader(obj_dat, batch_size=ntrn[i], shuffle=True, pin_memory=True, num_workers=NUM_WORKERS))\n",
    "\n",
    "    obj_dat_tst = md.MVTEC(root=\"./mvtec\", train=False, transform=transform, resize=IMSHAPE, interpolation=3, category=obj)\n",
    "    obj_data_tst_loader_eval.append(torch.utils.data.DataLoader(obj_dat_tst, batch_size=ntst[i], shuffle=True, pin_memory=True, num_workers=NUM_WORKERS))\n",
    "    \n",
    "\n",
    "    \n",
    "# Texture Eval\n",
    "texts = ['carpet', 'leather', 'wood']\n",
    "ntrn = [280,245,247]\n",
    "ntst = [117,124,79]\n",
    "\n",
    "text_data_loader_eval = list()\n",
    "text_data_tst_loader_eval = list()\n",
    "\n",
    "for (i,text) in enumerate(texts):\n",
    "    text_dat = md.MVTEC(root=\"./mvtec\", train=True, transform=transform, resize=IMSHAPE, interpolation=3, category=text)\n",
    "    text_data_loader_eval.append(torch.utils.data.DataLoader(text_dat, batch_size=ntrn[i], shuffle=True, pin_memory=True, num_workers=NUM_WORKERS))\n",
    "\n",
    "    text_dat_tst = md.MVTEC(root=\"./mvtec\", train=False, transform=transform, resize=IMSHAPE, interpolation=3, category=text)\n",
    "    text_data_tst_loader_eval.append(torch.utils.data.DataLoader(text_dat_tst, batch_size=ntst[i], shuffle=True, pin_memory=True, num_workers=NUM_WORKERS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of repetitions per experiment\n",
    "# Results reported in paper are the summary statistics over 10 repetitions (i.e., n_reps = 10)\n",
    "n_reps = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DOC (HINGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 1000\n",
    "\n",
    "logging.basicConfig(filename='MVTec_tile_DOC.log', level=logging.INFO)\n",
    "\n",
    "for lam in [0.01]:\n",
    "    for lr in [1e-5]:\n",
    "            \n",
    "        eps = 0.0\n",
    "        dpos = 1.0-eps\n",
    "        dneg = -(1.0+eps)\n",
    "            \n",
    "        print('================================')\n",
    "        print('lam = {}, lr = {}'.format(lam,lr))\n",
    "            \n",
    "\n",
    "        logging.info('================================')\n",
    "        logging.info('lam = {}, lr = {}'.format(lam,lr))\n",
    "\n",
    "        for repetition in range(n_reps):\n",
    "\n",
    "            print('--------------------------------')\n",
    "            print('repetition number: ')\n",
    "            print(repetition)\n",
    "\n",
    "            logging.info('--------------------------------')\n",
    "            logging.info('repetition number: ')\n",
    "            logging.info(repetition)\n",
    "\n",
    "            net = CIFAR10_LeNet()\n",
    "            net = net.to(device)\n",
    "\n",
    "            optimizer = torch.optim.Adam(net.parameters(), lr = lr)  \n",
    "            train_loss = HingeLoss()\n",
    "\n",
    "            for epoch in range(EPOCH+1):\n",
    "                    \n",
    "                loss_epoch = 0.0\n",
    "                n_batches = 0\n",
    "                epoch_start_time = time.time()\n",
    "\n",
    "                for (i,data) in enumerate(train_loader):\n",
    "                    inputs, labels = data\n",
    "                    inputs = inputs.to(device)\n",
    "                    ytr = np.ones((len(inputs),1))\n",
    "                    ytr = torch.from_numpy(ytr).to(device).float()\n",
    "                    outputs = net(inputs) \n",
    "\n",
    "                    # Zero the network parameter gradients\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # Loss\n",
    "                    loss = train_loss(outputs,ytr,smooth=True)\n",
    "\n",
    "                    lam = torch.tensor(lam).to(device)\n",
    "                    l2_reg = torch.tensor(0.).to(device)\n",
    "\n",
    "                    for name, param in net.named_parameters():\n",
    "                        l2_reg += torch.norm(param)**2\n",
    "\n",
    "                    loss += lam * l2_reg\n",
    "\n",
    "\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    loss_epoch += loss.item()\n",
    "                    n_batches += 1\n",
    "\n",
    "                if epoch % 200 == 0:\n",
    "                    idx_label_score = []\n",
    "                    net.eval()\n",
    "                    with torch.no_grad():\n",
    "\n",
    "                        for data in test_loader:\n",
    "                            inputs, labels = data\n",
    "\n",
    "                            labels = labels.cpu().data.numpy()\n",
    "                                \n",
    "                            inputs = inputs.to(device)\n",
    "                            outputs = net(inputs)\n",
    "                            scores = outputs.data.cpu().numpy()\n",
    "                            idx_label_score += list(zip(labels.tolist(),\n",
    "                                                        scores.tolist()))\n",
    "\n",
    "                    tstlabels, scores = zip(*idx_label_score)\n",
    "                    tstlabels = np.array(tstlabels)\n",
    "                    tstlabels[np.where(tstlabels==0)]=-1.0\n",
    "                    scores = np.array(scores)\n",
    "                    Tst_auc_score = evalmetrics(tstlabels,scores.flatten())   \n",
    "\n",
    "                    # log epoch statistics\n",
    "                    epoch_train_time = time.time() - epoch_start_time\n",
    "                    print('  Epoch {}/{}\\t Time: {:.3f}\\t Loss: {:.8f} \\t Test AUC :{:.4f} '\n",
    "                                .format(epoch , EPOCH, epoch_train_time, loss_epoch / n_batches, Tst_auc_score))\n",
    "                    logging.info('  Epoch {}/{}\\t Time: {:.3f}\\t Loss: {:.8f} \\t Test AUC :{:.4f} '\n",
    "                                .format(epoch , EPOCH, epoch_train_time, loss_epoch / n_batches, Tst_auc_score))\n",
    "\n",
    "            # Final Solution\n",
    "            print(' Final (TOT. EPOCHS {})::  Loss: {:.8f} \\t   AUC (Test) :{:.4f} '\n",
    "                    .format(EPOCH, loss_epoch / n_batches, Tst_auc_score))\n",
    "            logging.info(' Final (TOT. EPOCHS {})::  Loss: {:.8f} \\t   AUC (Test) :{:.4f} '\n",
    "                    .format(EPOCH, loss_epoch / n_batches, Tst_auc_score))\n",
    "                \n",
    "                \n",
    "            # CORRELATION VALUES\n",
    "            sampled_data = iter(train_loader_eval)\n",
    "            trinputs,_= sampled_data.next()\n",
    "                \n",
    "                \n",
    "            # UNIV NOISE\n",
    "            uinputs = torch.from_numpy(np.random.rand(trinputs.shape[0], 3, 64, 64)) \n",
    "            CORR_vals0 = SigmaTh2(trinputs, uinputs , net, feat = 'none')\n",
    "            CORR_vals1 = SigmaTh2(trinputs, uinputs , net, feat = 'cnn')\n",
    "            print('Correlation Vals (NOISE) = {}, {}'.format(CORR_vals0[0], CORR_vals1[0]))\n",
    "            logging.info('Correlation Vals (NOISE) = {}, {}'.format(CORR_vals0[0], CORR_vals1[0]))\n",
    "                \n",
    "            # UNIV OBJs\n",
    "            for (i,obj) in enumerate(objs): \n",
    "                sampled_data = iter(obj_data_loader_eval[i])\n",
    "                XU1, _ = sampled_data.next()\n",
    "                sampled_data = iter(obj_data_tst_loader_eval[i])\n",
    "                XU2,_ = sampled_data.next()\n",
    "                if i==0:\n",
    "                    uinputs = torch.cat((XU1,XU2))\n",
    "                else:\n",
    "                    uinputs = torch.cat((uinputs,XU1,XU2))\n",
    "                    \n",
    "            CORR_vals0 = SigmaTh2(trinputs, uinputs , net, feat = 'none')\n",
    "            CORR_vals1 = SigmaTh2(trinputs, uinputs , net, feat = 'cnn')\n",
    "            print('Correlation Vals (OBJECTS) = {}, {}'.format(CORR_vals0[0], CORR_vals1[0]))\n",
    "            logging.info('Correlation Vals (OBJECTS) = {}, {}'.format(CORR_vals0[0], CORR_vals1[0]))\n",
    "                \n",
    "            # UNIV TEXT\n",
    "            for (i,text) in enumerate(texts): \n",
    "                sampled_data = iter(text_data_loader_eval[i])\n",
    "                XU1, _ = sampled_data.next()\n",
    "                sampled_data = iter(text_data_tst_loader_eval[i])\n",
    "                XU2,_ = sampled_data.next()\n",
    "                if i==0:\n",
    "                    uinputs = torch.cat((XU1,XU2))\n",
    "                else:\n",
    "                    uinputs = torch.cat((uinputs,XU1,XU2))\n",
    "                \n",
    "            CORR_vals0 = SigmaTh2(trinputs, uinputs , net, feat = 'none')\n",
    "            CORR_vals1 = SigmaTh2(trinputs, uinputs , net, feat = 'cnn')\n",
    "            print('Correlation Vals (Texture) = {}, {}'.format(CORR_vals0[0], CORR_vals1[0]))\n",
    "            logging.info('Correlation Vals (Texture) = {}, {}'.format(CORR_vals0[0], CORR_vals1[0]))\n",
    "                \n",
    "            print('----------------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DOC3 (Noise Universum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 1000\n",
    "\n",
    "logging.basicConfig(filename='MVTec_tile_DOC3_Univ_noise.log', level=logging.INFO)\n",
    "\n",
    "for lam in [0.1]:\n",
    "    for lr in [5e-6]:\n",
    "        for Cu in [0.1]:\n",
    "            \n",
    "            eps = 0.0\n",
    "            dpos = 1.0-eps\n",
    "            dneg = -(1.0+eps)\n",
    "            \n",
    "            print('================================')\n",
    "            print('lam = {}, lr = {}, Cu = {}'.format(lam,lr,Cu))\n",
    "            \n",
    "\n",
    "            logging.info('================================')\n",
    "            logging.info('lam = {}, lr = {}, Cu = {}'.format(lam,lr,Cu))\n",
    "\n",
    "            for repetition in range(n_reps):\n",
    "\n",
    "                print('--------------------------------')\n",
    "                print('repetition number: ')\n",
    "                print(repetition)\n",
    "\n",
    "                logging.info('--------------------------------')\n",
    "                logging.info('repetition number: ')\n",
    "                logging.info(repetition)\n",
    "\n",
    "                net = CIFAR10_LeNet()\n",
    "                net = net.to(device)\n",
    "\n",
    "                optimizer = torch.optim.Adam(net.parameters(), lr = lr)  \n",
    "                train_loss = HingeLoss()\n",
    "                unlabeled_posloss = HingeLoss()\n",
    "                unlabeled_negloss = HingeLoss()\n",
    "\n",
    "                for epoch in range(EPOCH+1):\n",
    "                    \n",
    "                    loss_epoch = 0.0\n",
    "                    n_batches = 0\n",
    "                    epoch_start_time = time.time()\n",
    "\n",
    "                    for (i,data) in enumerate(train_loader):\n",
    "                        inputs, labels = data\n",
    "                        inputs = inputs.to(device)\n",
    "                        ytr = np.ones((len(inputs),1))\n",
    "                        ytr = torch.from_numpy(ytr).to(device).float()\n",
    "                        outputs = net(inputs) \n",
    "                        \n",
    "                        XU = torch.from_numpy(np.random.rand(BATCH_SIZE, 3, 64, 64))\n",
    "                                                \n",
    "                        XU = XU.to(device).float()\n",
    "                        outputsU = net(XU)\n",
    "                        yunppos = np.ones(BATCH_SIZE)\n",
    "                        yunpneg = -np.ones(BATCH_SIZE)\n",
    "                        yupos = torch.from_numpy(yunppos).to(device).float()\n",
    "                        yuneg = torch.from_numpy(yunpneg).to(device).float()\n",
    "\n",
    "                        # Zero the network parameter gradients\n",
    "                        optimizer.zero_grad()\n",
    "\n",
    "                        # Loss\n",
    "                        loss = train_loss(outputs,ytr,smooth=True)\n",
    "\n",
    "                        # Unlabeled data\n",
    "                        lossunlab = unlabeled_posloss(outputsU,yupos,dpos) + unlabeled_negloss(outputsU,yuneg,dneg)\n",
    "                        loss+=Cu*lossunlab\n",
    "\n",
    "                        lam = torch.tensor(lam).to(device)\n",
    "                        l2_reg = torch.tensor(0.).to(device)\n",
    "\n",
    "                        for name, param in net.named_parameters():\n",
    "                            l2_reg += torch.norm(param)**2\n",
    "\n",
    "                        loss += lam * l2_reg\n",
    "\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                        loss_epoch += loss.item()\n",
    "                        n_batches += 1\n",
    "\n",
    "                    if epoch % 200 == 0:\n",
    "                        idx_label_score = []\n",
    "                        net.eval()\n",
    "                        with torch.no_grad():\n",
    "\n",
    "                            for data in test_loader:\n",
    "                                inputs, labels = data\n",
    "\n",
    "                                labels = labels.cpu().data.numpy()\n",
    "                                \n",
    "                                inputs = inputs.to(device)\n",
    "                                outputs = net(inputs)\n",
    "                                scores = outputs.data.cpu().numpy()\n",
    "                                idx_label_score += list(zip(labels.tolist(),\n",
    "                                                            scores.tolist()))\n",
    "\n",
    "                        tstlabels, scores = zip(*idx_label_score)\n",
    "                        tstlabels = np.array(tstlabels)\n",
    "                        tstlabels[np.where(tstlabels==0)]=-1.0\n",
    "                        scores = np.array(scores)\n",
    "                        Tst_auc_score = evalmetrics(tstlabels,scores.flatten()) \n",
    "\n",
    "                        # log epoch statistics\n",
    "                        epoch_train_time = time.time() - epoch_start_time\n",
    "                        print('  Epoch {}/{}\\t Time: {:.3f}\\t Loss: {:.8f} \\t Test AUC :{:.4f} '\n",
    "                                    .format(epoch , EPOCH, epoch_train_time, loss_epoch / n_batches, Tst_auc_score))\n",
    "                        logging.info('  Epoch {}/{}\\t Time: {:.3f}\\t Loss: {:.8f} \\t Test AUC :{:.4f} '\n",
    "                                    .format(epoch , EPOCH, epoch_train_time, loss_epoch / n_batches, Tst_auc_score))\n",
    "\n",
    "                # Final Solution\n",
    "                print(' Final (TOT. EPOCHS {})::  Loss: {:.8f} \\t   AUC (Test) :{:.4f} '\n",
    "                        .format(EPOCH, loss_epoch / n_batches, Tst_auc_score))\n",
    "                logging.info(' Final (TOT. EPOCHS {})::  Loss: {:.8f} \\t   AUC (Test) :{:.4f} '\n",
    "                        .format(EPOCH, loss_epoch / n_batches, Tst_auc_score))\n",
    "                \n",
    "                \n",
    "                # CORRELATION VALUES\n",
    "                sampled_data = iter(train_loader_eval)\n",
    "                trinputs,_= sampled_data.next()\n",
    "                \n",
    "                \n",
    "                # UNIV NOISE\n",
    "                uinputs = torch.from_numpy(np.random.rand(trinputs.shape[0], 3, 64, 64)) \n",
    "                CORR_vals0 = SigmaTh2(trinputs, uinputs , net, feat = 'none')\n",
    "                CORR_vals1 = SigmaTh2(trinputs, uinputs , net, feat = 'cnn')\n",
    "                print('Correlation Vals (NOISE) = {}, {}'.format(CORR_vals0[0], CORR_vals1[0]))\n",
    "                logging.info('Correlation Vals (NOISE) = {}, {}'.format(CORR_vals0[0], CORR_vals1[0]))\n",
    "                \n",
    "                \n",
    "                print('----------------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DOC3 (Objects Universum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "univ_data_loader = list()\n",
    "univ_data_tst_loader = list()\n",
    "\n",
    "for (i,obj) in enumerate(objs):\n",
    "    univ_dat = md.MVTEC(root=\"./mvtec\", train=True, transform=transform, resize=IMSHAPE, interpolation=3,category=obj)\n",
    "    univ_data_loader.append(torch.utils.data.DataLoader(univ_dat, batch_size=int(BATCH_SIZE/2), shuffle=True, pin_memory=True, num_workers=NUM_WORKERS))\n",
    "\n",
    "    univ_dat_tst = md.MVTEC(root=\"./mvtec\", train=False, transform=transform, resize=IMSHAPE, interpolation=3,category=obj)\n",
    "    univ_data_tst_loader.append(torch.utils.data.DataLoader(univ_dat_tst, batch_size=int(BATCH_SIZE/2), shuffle=True, pin_memory=True, num_workers=NUM_WORKERS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 1000\n",
    "\n",
    "logging.basicConfig(filename='MVTec_tile_DOC3_Univ_objects.log', level=logging.INFO)\n",
    "\n",
    "for lam in [0.005]:\n",
    "    for lr in [1e-4]:\n",
    "        for Cu in [2.0]:\n",
    "            \n",
    "            eps = 0.0\n",
    "            dpos = 1.0-eps\n",
    "            dneg = -(1.0+eps)\n",
    "            \n",
    "            print('================================')\n",
    "            print('lam = {}, lr = {}, Cu = {}'.format(lam,lr,Cu))\n",
    "            \n",
    "\n",
    "            logging.info('================================')\n",
    "            logging.info('lam = {}, lr = {}, Cu = {}'.format(lam,lr,Cu))\n",
    "\n",
    "            for repetition in range(n_reps):\n",
    "\n",
    "                print('--------------------------------')\n",
    "                print('repetition number: ')\n",
    "                print(repetition)\n",
    "\n",
    "                logging.info('--------------------------------')\n",
    "                logging.info('repetition number: ')\n",
    "                logging.info(repetition)\n",
    "\n",
    "                net = CIFAR10_LeNet()\n",
    "                net = net.to(device)\n",
    "\n",
    "                optimizer = torch.optim.Adam(net.parameters(), lr = lr)  \n",
    "                train_loss = HingeLoss()\n",
    "                unlabeled_posloss = HingeLoss()\n",
    "                unlabeled_negloss = HingeLoss()\n",
    "\n",
    "                for epoch in range(EPOCH+1):\n",
    "                    \n",
    "                    loss_epoch = 0.0\n",
    "                    n_batches = 0\n",
    "                    epoch_start_time = time.time()\n",
    "\n",
    "                    for (i,data) in enumerate(train_loader):\n",
    "                        inputs, labels = data\n",
    "                        inputs = inputs.to(device)\n",
    "                        ytr = np.ones((len(inputs),1))\n",
    "                        ytr = torch.from_numpy(ytr).to(device).float()\n",
    "                        outputs = net(inputs) \n",
    "                        \n",
    "                        uint = np.random.randint(0, 7)\n",
    "                        \n",
    "                        sampled_data = iter(univ_data_loader[uint])\n",
    "                        XU1, _ = sampled_data.next()\n",
    "                        sampled_data = iter(univ_data_tst_loader[uint])\n",
    "                        XU2,_ = sampled_data.next()\n",
    "                        XU = torch.cat((XU1,XU2))\n",
    "                                              \n",
    "                        XU = XU.to(device).float()\n",
    "                        outputsU = net(XU)\n",
    "                        yunppos = np.ones(BATCH_SIZE)\n",
    "                        yunpneg = -np.ones(BATCH_SIZE)\n",
    "                        yupos = torch.from_numpy(yunppos).to(device).float()\n",
    "                        yuneg = torch.from_numpy(yunpneg).to(device).float()\n",
    "\n",
    "                        # Zero the network parameter gradients\n",
    "                        optimizer.zero_grad()\n",
    "\n",
    "                        # Loss\n",
    "                        loss = train_loss(outputs,ytr,smooth = True)\n",
    "\n",
    "                        # Unlabeled data\n",
    "                        lossunlab = unlabeled_posloss(outputsU,yupos,dpos) + unlabeled_negloss(outputsU,yuneg,dneg)\n",
    "                        loss+=Cu*lossunlab\n",
    "\n",
    "                        lam = torch.tensor(lam).to(device)\n",
    "                        l2_reg = torch.tensor(0.).to(device)\n",
    "\n",
    "                        for name, param in net.named_parameters():\n",
    "                            l2_reg += torch.norm(param)**2\n",
    "\n",
    "                        loss += lam * l2_reg\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                        loss_epoch += loss.item()\n",
    "                        n_batches += 1\n",
    "\n",
    "                    if epoch % 200 == 0:\n",
    "                        idx_label_score = []\n",
    "                        net.eval()\n",
    "                        with torch.no_grad():\n",
    "\n",
    "                            for data in test_loader:\n",
    "                                inputs, labels = data\n",
    "\n",
    "                                labels = labels.cpu().data.numpy()\n",
    "                                \n",
    "                                inputs = inputs.to(device)\n",
    "                                outputs = net(inputs)\n",
    "                                scores = outputs.data.cpu().numpy()\n",
    "                                idx_label_score += list(zip(labels.tolist(),\n",
    "                                                            scores.tolist()))\n",
    "\n",
    "                        tstlabels, scores = zip(*idx_label_score)\n",
    "                        tstlabels = np.array(tstlabels)\n",
    "                        tstlabels[np.where(tstlabels==0)]=-1.0\n",
    "                        scores = np.array(scores)\n",
    "                        Tst_auc_score = evalmetrics(tstlabels,scores.flatten())\n",
    "                        \n",
    "                        # log epoch statistics\n",
    "                        epoch_train_time = time.time() - epoch_start_time\n",
    "                        print('  Epoch {}/{}\\t Time: {:.3f}\\t Loss: {:.8f} \\t Test AUC :{:.4f} '\n",
    "                                    .format(epoch , EPOCH, epoch_train_time, loss_epoch / n_batches, Tst_auc_score))\n",
    "                        logging.info('  Epoch {}/{}\\t Time: {:.3f}\\t Loss: {:.8f} \\t Test AUC :{:.4f} '\n",
    "                                    .format(epoch , EPOCH, epoch_train_time, loss_epoch / n_batches, Tst_auc_score))\n",
    "\n",
    "                # Final Solution\n",
    "                print(' Final (TOT. EPOCHS {})::  Loss: {:.8f} \\t   AUC (Test) :{:.4f} '\n",
    "                        .format(EPOCH, loss_epoch / n_batches, Tst_auc_score))\n",
    "                logging.info(' Final (TOT. EPOCHS {})::  Loss: {:.8f} \\t   AUC (Test) :{:.4f} '\n",
    "                        .format(EPOCH, loss_epoch / n_batches, Tst_auc_score))\n",
    "\n",
    "                \n",
    "                # CORRELATION VALUES\n",
    "                sampled_data = iter(train_loader_eval)\n",
    "                trinputs,_= sampled_data.next()\n",
    "                \n",
    "                \n",
    "                \n",
    "                # UNIV OBJs\n",
    "                for (i,obj) in enumerate(objs): \n",
    "                    sampled_data = iter(obj_data_loader_eval[i])\n",
    "                    XU1, _ = sampled_data.next()\n",
    "                    sampled_data = iter(obj_data_tst_loader_eval[i])\n",
    "                    XU2,_ = sampled_data.next()\n",
    "                    if i==0:\n",
    "                        uinputs = torch.cat((XU1,XU2))\n",
    "                    else:\n",
    "                        uinputs = torch.cat((uinputs,XU1,XU2))\n",
    "                    \n",
    "                CORR_vals0 = SigmaTh2(trinputs, uinputs , net, feat = 'none')\n",
    "                CORR_vals1 = SigmaTh2(trinputs, uinputs , net, feat = 'cnn')\n",
    "                print('Correlation Vals (OBJECTS) = {}, {}'.format(CORR_vals0[0], CORR_vals1[0]))\n",
    "                logging.info('Correlation Vals (OBJECTS) = {}, {}'.format(CORR_vals0[0], CORR_vals1[0]))               \n",
    "                \n",
    "                print('----------------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DOC3 (Textures Universum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "univ_data_loader = list()\n",
    "univ_data_tst_loader = list()\n",
    "\n",
    "for (i,txt) in enumerate(texts):\n",
    "    univ_dat = md.MVTEC(root=\"./mvtec\", train=True, transform=transform, resize=IMSHAPE, interpolation=3,category=txt)\n",
    "    univ_data_loader.append(torch.utils.data.DataLoader(univ_dat, batch_size=int(BATCH_SIZE/2), shuffle=True, pin_memory=True, num_workers=NUM_WORKERS))\n",
    "\n",
    "    univ_dat_tst = md.MVTEC(root=\"./mvtec\", train=False, transform=transform, resize=IMSHAPE, interpolation=3,category=txt)\n",
    "    univ_data_tst_loader.append(torch.utils.data.DataLoader(univ_dat_tst, batch_size=int(BATCH_SIZE/2), shuffle=True, pin_memory=True, num_workers=NUM_WORKERS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 1000 \n",
    "\n",
    "logging.basicConfig(filename='MVTec_tile_DOC3_Univ_textures.log', level=logging.INFO)\n",
    "\n",
    "for lam in [0.1]:\n",
    "    for lr in [5e-6]:\n",
    "        for Cu in [0.1]:\n",
    "            \n",
    "            eps = 0.0\n",
    "            dpos = 1.0-eps\n",
    "            dneg = -(1.0+eps)\n",
    "            \n",
    "            print('================================')\n",
    "            print('lam = {}, lr = {}, Cu = {}'.format(lam,lr,Cu))\n",
    "            \n",
    "\n",
    "            logging.info('================================')\n",
    "            logging.info('lam = {}, lr = {}, Cu = {}'.format(lam,lr,Cu))\n",
    "\n",
    "            for repetition in range(n_reps):\n",
    "\n",
    "                print('--------------------------------')\n",
    "                print('repetition number: ')\n",
    "                print(repetition)\n",
    "\n",
    "                logging.info('--------------------------------')\n",
    "                logging.info('repetition number: ')\n",
    "                logging.info(repetition)\n",
    "\n",
    "                net = CIFAR10_LeNet()\n",
    "                net = net.to(device)\n",
    "\n",
    "                optimizer = torch.optim.Adam(net.parameters(), lr = lr)  \n",
    "                train_loss = HingeLoss()\n",
    "                unlabeled_posloss = HingeLoss()\n",
    "                unlabeled_negloss = HingeLoss()\n",
    "\n",
    "                for epoch in range(EPOCH+1):\n",
    "                    \n",
    "                    loss_epoch = 0.0\n",
    "                    n_batches = 0\n",
    "                    epoch_start_time = time.time()\n",
    "\n",
    "                    for (i,data) in enumerate(train_loader):\n",
    "                        inputs, labels = data\n",
    "                        inputs = inputs.to(device)\n",
    "                        ytr = np.ones((len(inputs),1))\n",
    "                        ytr = torch.from_numpy(ytr).to(device).float()\n",
    "                        outputs = net(inputs) \n",
    "                        \n",
    "                        uint = np.random.randint(0, 3)\n",
    "                        \n",
    "                        sampled_data = iter(univ_data_loader[uint])\n",
    "                        XU1, _ = sampled_data.next()\n",
    "                        sampled_data = iter(univ_data_tst_loader[uint])\n",
    "                        XU2,_ = sampled_data.next()\n",
    "                        XU = torch.cat((XU1,XU2))\n",
    "                                             \n",
    "                        XU = XU.to(device).float()\n",
    "                        outputsU = net(XU)\n",
    "                        yunppos = np.ones(BATCH_SIZE)\n",
    "                        yunpneg = -np.ones(BATCH_SIZE)\n",
    "                        yupos = torch.from_numpy(yunppos).to(device).float()\n",
    "                        yuneg = torch.from_numpy(yunpneg).to(device).float()\n",
    "\n",
    "                        # Zero the network parameter gradients\n",
    "                        optimizer.zero_grad()\n",
    "\n",
    "                        # Loss\n",
    "                        loss = train_loss(outputs,ytr,smooth = True)\n",
    "\n",
    "                        # Unlabeled data\n",
    "                        lossunlab = unlabeled_posloss(outputsU,yupos,dpos) + unlabeled_negloss(outputsU,yuneg,dneg)\n",
    "                        loss+=Cu*lossunlab\n",
    "\n",
    "                        lam = torch.tensor(lam).to(device)\n",
    "                        l2_reg = torch.tensor(0.).to(device)\n",
    "\n",
    "                        for name, param in net.named_parameters():\n",
    "                            l2_reg += torch.norm(param)**2\n",
    "\n",
    "                        loss += lam * l2_reg\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                        loss_epoch += loss.item()\n",
    "                        n_batches += 1\n",
    "\n",
    "                    if epoch % 200 == 0:\n",
    "                        idx_label_score = []\n",
    "                        net.eval()\n",
    "                        with torch.no_grad():\n",
    "\n",
    "                            for data in test_loader:\n",
    "                                inputs, labels = data\n",
    "\n",
    "                                labels = labels.cpu().data.numpy()\n",
    "\n",
    "                                inputs = inputs.to(device)\n",
    "                                outputs = net(inputs)\n",
    "                                scores = outputs.data.cpu().numpy()\n",
    "                                idx_label_score += list(zip(labels.tolist(),\n",
    "                                                            scores.tolist()))\n",
    "\n",
    "                        tstlabels, scores = zip(*idx_label_score)\n",
    "                        tstlabels = np.array(tstlabels)\n",
    "                        tstlabels[np.where(tstlabels==0)]=-1.0\n",
    "                        scores = np.array(scores)\n",
    "                        Tst_auc_score = evalmetrics(tstlabels,scores.flatten())\n",
    "                        \n",
    "                        # log epoch statistics\n",
    "                        epoch_train_time = time.time() - epoch_start_time\n",
    "                        print('  Epoch {}/{}\\t Time: {:.3f}\\t Loss: {:.8f} \\t Test AUC :{:.4f} '\n",
    "                                    .format(epoch , EPOCH, epoch_train_time, loss_epoch / n_batches, Tst_auc_score))\n",
    "                        logging.info('  Epoch {}/{}\\t Time: {:.3f}\\t Loss: {:.8f} \\t Test AUC :{:.4f} '\n",
    "                                    .format(epoch , EPOCH, epoch_train_time, loss_epoch / n_batches, Tst_auc_score))\n",
    "\n",
    "                # Final Solution\n",
    "                print(' Final (TOT. EPOCHS {})::  Loss: {:.8f} \\t   AUC (Test) :{:.4f} '\n",
    "                        .format(EPOCH, loss_epoch / n_batches, Tst_auc_score))\n",
    "                logging.info(' Final (TOT. EPOCHS {})::  Loss: {:.8f} \\t   AUC (Test) :{:.4f} '\n",
    "                        .format(EPOCH, loss_epoch / n_batches, Tst_auc_score))\n",
    "                \n",
    "                \n",
    "                # CORRELATION VALUES\n",
    "                sampled_data = iter(train_loader_eval)\n",
    "                trinputs,_= sampled_data.next()\n",
    "                 \n",
    "                # UNIV TEXT\n",
    "                for (i,text) in enumerate(texts): \n",
    "                    sampled_data = iter(text_data_loader_eval[i])\n",
    "                    XU1, _ = sampled_data.next()\n",
    "                    sampled_data = iter(text_data_tst_loader_eval[i])\n",
    "                    XU2,_ = sampled_data.next()\n",
    "                    if i==0:\n",
    "                        uinputs = torch.cat((XU1,XU2))\n",
    "                    else:\n",
    "                        uinputs = torch.cat((uinputs,XU1,XU2))\n",
    "                \n",
    "                CORR_vals0 = SigmaTh2(trinputs, uinputs , net, feat = 'none')\n",
    "                CORR_vals1 = SigmaTh2(trinputs, uinputs , net, feat = 'cnn')\n",
    "                print('Correlation Vals (Texture) = {}, {}'.format(CORR_vals0[0], CORR_vals1[0]))\n",
    "                logging.info('Correlation Vals (Texture) = {}, {}'.format(CORR_vals0[0], CORR_vals1[0]))\n",
    "                \n",
    "                \n",
    "                print('----------------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
