{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep One Class Classification (DOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import logging\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import BatchSampler\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For code optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of workers:  6\n"
     ]
    }
   ],
   "source": [
    "# uncomment for code optimization\n",
    "#torch.backends.cudnn.benchmark = True\n",
    "\n",
    "NUM_WORKERS = int(os.cpu_count() / 2)\n",
    "print('number of workers: ', NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a. Evaluation (Loss, Eval Metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function\n",
    "class HingeLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HingeLoss, self).__init__()\n",
    "    \n",
    "    def forward(self, ypred, ytrue, margin = 1.0, smooth = False):\n",
    "        ypred = ypred.squeeze()\n",
    "        if smooth:\n",
    "            loss = torch.nn.Softplus()\n",
    "            out = torch.mean(loss(margin - (ytrue * ypred)))\n",
    "        else:\n",
    "            out = torch.mean(torch.relu(margin - (ytrue * ypred)))\n",
    "        return out\n",
    "\n",
    "\n",
    "def evalmetrics(y_true, scores):\n",
    "    auc_score = roc_auc_score(y_true, scores)\n",
    "    \n",
    "    return auc_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b. Data Set Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BalancedBatchSampler(BatchSampler): \n",
    "    \"\"\"\n",
    "    BatchSampler - from a MNIST-like dataset, samples n_classes and within these classes samples n_samples.\n",
    "    Returns batches of size n_classes * n_samples\n",
    "    # CODE SOURCE : https://discuss.pytorch.org/t/load-the-same-number-of-data-per-class/65198/4\n",
    "    # LICENSE: NOT AVAILABLE!\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, n_classes, n_samples, class_id = None, allSamples = False):\n",
    "        loader = torch.utils.data.DataLoader(dataset)\n",
    "        self.labels_list = []\n",
    "        \n",
    "        for _, label in loader:\n",
    "            self.labels_list.append(label)\n",
    "        \n",
    "        self.labels = torch.LongTensor(self.labels_list)\n",
    "        self.labels_set = list(set(self.labels.numpy()))\n",
    "        print(self.labels_set)\n",
    "        \n",
    "        self.label_to_indices = {label: np.where(self.labels.numpy() == label)[0]\n",
    "                                 for label in self.labels_set}\n",
    "        \n",
    "        for l in self.labels_set:\n",
    "            np.random.shuffle(self.label_to_indices[l])\n",
    "            \n",
    "        self.used_label_indices_count = {label: 0 for label in self.labels_set}\n",
    "        self.count = 0\n",
    "        self.n_classes = n_classes\n",
    "        self.n_samples = n_samples\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = self.n_samples * self.n_classes\n",
    "        self.class_id = class_id\n",
    "        self.allSamples = allSamples\n",
    " \n",
    "    def __iter__(self):\n",
    "        self.count = 0\n",
    "        while self.count + self.batch_size < len(self.dataset):\n",
    "            \n",
    "            if self.class_id is not None:\n",
    "                classes = self.class_id\n",
    "            else:\n",
    "                classes = np.random.choice(self.labels_set, self.n_classes, replace=False)\n",
    "            \n",
    "            indices = []\n",
    "            for class_ in classes:\n",
    "                if not self.allSamples: \n",
    "                    indices.extend(self.label_to_indices[class_][\n",
    "                                   self.used_label_indices_count[class_]:self.used_label_indices_count[\n",
    "                                                                        class_] + self.n_samples])\n",
    "                else: \n",
    "                    indices.extend(self.label_to_indices[class_])\n",
    "                \n",
    "                self.used_label_indices_count[class_] += self.n_samples\n",
    "                \n",
    "                if self.used_label_indices_count[class_] + self.n_samples > len(self.label_to_indices[class_]):\n",
    "                    np.random.shuffle(self.label_to_indices[class_])\n",
    "                    self.used_label_indices_count[class_] = 0\n",
    "                    \n",
    "            yield indices\n",
    "            \n",
    "            self.count += self.n_classes * self.n_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset) // self.batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10_LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CIFAR10_LeNet, self).__init__()\n",
    "\n",
    "        self.rep_dim = 128\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 32, 5, bias=False, padding=2)\n",
    "        self.bn2d1 = nn.BatchNorm2d(32, eps=1e-04, affine=False)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 5, bias=False, padding=2)\n",
    "        self.bn2d2 = nn.BatchNorm2d(64, eps=1e-04, affine=False)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 5, bias=False, padding=2)\n",
    "        self.bn2d3 = nn.BatchNorm2d(128, eps=1e-04, affine=False)\n",
    "        self.fc1 = nn.Linear(128 * 4 * 4, self.rep_dim, bias=False)\n",
    "        self.fc2 = nn.Linear(self.rep_dim, int(self.rep_dim/2), bias=False)\n",
    "        self.fc3 = nn.Linear(int(self.rep_dim/2), 1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool(F.leaky_relu(self.bn2d1(x)))\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool(F.leaky_relu(self.bn2d2(x)))\n",
    "        x = self.conv3(x)\n",
    "        x = self.pool(F.leaky_relu(self.bn2d3(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training Schedule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All 10 Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Files already downloaded and verified\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "----------------------------------------------\n",
      "Class: 0 lam: 0.5 # epochs: 300 lr: 0.005\n",
      "**********************************\n",
      "repetition number: 0\n",
      "  Epoch 1/300\t Time: 3.785\t Loss: 64.07535754 \t Test AUC :0.4961 \n",
      "  Epoch 51/300\t Time: 1.004\t Loss: 0.17551526 \t Test AUC :0.7663 \n",
      "  Epoch 101/300\t Time: 0.918\t Loss: 0.17170373 \t Test AUC :0.7697 \n",
      "  Epoch 151/300\t Time: 0.911\t Loss: 0.17140419 \t Test AUC :0.7783 \n",
      "  Epoch 201/300\t Time: 1.047\t Loss: 0.17258948 \t Test AUC :0.7727 \n",
      "  Epoch 251/300\t Time: 0.955\t Loss: 0.17273657 \t Test AUC :0.7778 \n",
      " Final (TOT. EPOCHS 300)::  Loss: 0.17065016 \t   AUC (Test) :0.7778 \n",
      "Files already downloaded and verified\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Files already downloaded and verified\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "----------------------------------------------\n",
      "Class: 1 lam: 1.0 # epochs: 300 lr: 0.001\n",
      "**********************************\n",
      "repetition number: 0\n",
      "  Epoch 1/300\t Time: 1.000\t Loss: 134.29507527 \t Test AUC :0.7121 \n",
      "  Epoch 51/300\t Time: 1.004\t Loss: 3.36408586 \t Test AUC :0.6990 \n",
      "  Epoch 101/300\t Time: 1.104\t Loss: 0.43629176 \t Test AUC :0.6837 \n",
      "  Epoch 151/300\t Time: 0.981\t Loss: 0.36639341 \t Test AUC :0.6813 \n",
      "  Epoch 201/300\t Time: 1.030\t Loss: 0.36327776 \t Test AUC :0.6953 \n",
      "  Epoch 251/300\t Time: 0.987\t Loss: 0.36226653 \t Test AUC :0.7046 \n",
      " Final (TOT. EPOCHS 300)::  Loss: 0.36184122 \t   AUC (Test) :0.7046 \n",
      "Files already downloaded and verified\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Files already downloaded and verified\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "----------------------------------------------\n",
      "Class: 2 lam: 0.5 # epochs: 300 lr: 0.005\n",
      "**********************************\n",
      "repetition number: 0\n",
      "  Epoch 1/300\t Time: 1.038\t Loss: 63.72582566 \t Test AUC :0.3850 \n",
      "  Epoch 51/300\t Time: 1.092\t Loss: 0.17961624 \t Test AUC :0.5238 \n",
      "  Epoch 101/300\t Time: 1.026\t Loss: 0.17308214 \t Test AUC :0.5277 \n",
      "  Epoch 151/300\t Time: 1.020\t Loss: 0.17417187 \t Test AUC :0.5216 \n",
      "  Epoch 201/300\t Time: 1.158\t Loss: 0.17184682 \t Test AUC :0.5281 \n",
      "  Epoch 251/300\t Time: 0.989\t Loss: 0.17389404 \t Test AUC :0.5273 \n",
      " Final (TOT. EPOCHS 300)::  Loss: 0.17249554 \t   AUC (Test) :0.5273 \n",
      "Files already downloaded and verified\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Files already downloaded and verified\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "----------------------------------------------\n",
      "Class: 3 lam: 1.0 # epochs: 300 lr: 0.001\n",
      "**********************************\n",
      "repetition number: 0\n",
      "  Epoch 1/300\t Time: 1.137\t Loss: 135.17751915 \t Test AUC :0.5104 \n",
      "  Epoch 51/300\t Time: 1.081\t Loss: 3.35644975 \t Test AUC :0.5060 \n",
      "  Epoch 101/300\t Time: 1.202\t Loss: 0.42082581 \t Test AUC :0.5928 \n",
      "  Epoch 151/300\t Time: 1.071\t Loss: 0.35218194 \t Test AUC :0.6058 \n",
      "  Epoch 201/300\t Time: 1.058\t Loss: 0.34915798 \t Test AUC :0.6071 \n",
      "  Epoch 251/300\t Time: 1.100\t Loss: 0.34844600 \t Test AUC :0.6064 \n",
      " Final (TOT. EPOCHS 300)::  Loss: 0.34792419 \t   AUC (Test) :0.6064 \n",
      "Files already downloaded and verified\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Files already downloaded and verified\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "----------------------------------------------\n",
      "Class: 4 lam: 1.0 # epochs: 300 lr: 0.001\n",
      "**********************************\n",
      "repetition number: 0\n",
      "  Epoch 1/300\t Time: 1.061\t Loss: 135.25213543 \t Test AUC :0.2805 \n",
      "  Epoch 51/300\t Time: 1.089\t Loss: 3.34545519 \t Test AUC :0.3111 \n",
      "  Epoch 101/300\t Time: 1.043\t Loss: 0.41105335 \t Test AUC :0.6243 \n",
      "  Epoch 151/300\t Time: 1.030\t Loss: 0.34330771 \t Test AUC :0.6325 \n",
      "  Epoch 201/300\t Time: 1.155\t Loss: 0.34097541 \t Test AUC :0.6242 \n",
      "  Epoch 251/300\t Time: 1.008\t Loss: 0.34054604 \t Test AUC :0.6198 \n",
      " Final (TOT. EPOCHS 300)::  Loss: 0.34035405 \t   AUC (Test) :0.6198 \n",
      "Files already downloaded and verified\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Files already downloaded and verified\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "----------------------------------------------\n",
      "Class: 5 lam: 0.5 # epochs: 400 lr: 0.001\n",
      "**********************************\n",
      "repetition number: 0\n",
      "  Epoch 1/400\t Time: 1.069\t Loss: 68.55739513 \t Test AUC :0.5594 \n",
      "  Epoch 51/400\t Time: 1.028\t Loss: 10.31861516 \t Test AUC :0.5356 \n",
      "  Epoch 101/400\t Time: 1.131\t Loss: 1.69774206 \t Test AUC :0.5460 \n",
      "  Epoch 151/400\t Time: 1.072\t Loss: 0.40494856 \t Test AUC :0.5832 \n",
      "  Epoch 201/400\t Time: 1.030\t Loss: 0.21040537 \t Test AUC :0.6088 \n",
      "  Epoch 251/400\t Time: 1.050\t Loss: 0.18083499 \t Test AUC :0.6218 \n",
      "  Epoch 301/400\t Time: 1.169\t Loss: 0.17600763 \t Test AUC :0.6269 \n",
      "  Epoch 351/400\t Time: 1.009\t Loss: 0.17520105 \t Test AUC :0.6254 \n",
      " Final (TOT. EPOCHS 400)::  Loss: 0.17474876 \t   AUC (Test) :0.6254 \n",
      "Files already downloaded and verified\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Files already downloaded and verified\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "----------------------------------------------\n",
      "Class: 6 lam: 0.5 # epochs: 300 lr: 0.001\n",
      "**********************************\n",
      "repetition number: 0\n",
      "  Epoch 1/300\t Time: 1.146\t Loss: 69.09942627 \t Test AUC :0.3212 \n",
      "  Epoch 51/300\t Time: 1.077\t Loss: 10.40795793 \t Test AUC :0.3014 \n",
      "  Epoch 101/300\t Time: 1.177\t Loss: 1.69904068 \t Test AUC :0.5501 \n",
      "  Epoch 151/300\t Time: 1.055\t Loss: 0.39556899 \t Test AUC :0.7486 \n",
      "  Epoch 201/300\t Time: 1.076\t Loss: 0.19958878 \t Test AUC :0.7718 \n",
      "  Epoch 251/300\t Time: 1.066\t Loss: 0.16938725 \t Test AUC :0.7675 \n",
      " Final (TOT. EPOCHS 300)::  Loss: 0.16424675 \t   AUC (Test) :0.7675 \n",
      "Files already downloaded and verified\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Files already downloaded and verified\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "----------------------------------------------\n",
      "Class: 7 lam: 1.0 # epochs: 300 lr: 0.001\n",
      "**********************************\n",
      "repetition number: 0\n",
      "  Epoch 1/300\t Time: 1.190\t Loss: 135.02215496 \t Test AUC :0.6209 \n",
      "  Epoch 51/300\t Time: 1.049\t Loss: 3.36418095 \t Test AUC :0.5678 \n",
      "  Epoch 101/300\t Time: 1.058\t Loss: 0.42821717 \t Test AUC :0.6114 \n",
      "  Epoch 151/300\t Time: 1.026\t Loss: 0.35920966 \t Test AUC :0.5952 \n",
      "  Epoch 201/300\t Time: 1.096\t Loss: 0.35658484 \t Test AUC :0.5939 \n",
      "  Epoch 251/300\t Time: 1.025\t Loss: 0.35615623 \t Test AUC :0.5940 \n",
      " Final (TOT. EPOCHS 300)::  Loss: 0.35595387 \t   AUC (Test) :0.5940 \n",
      "Files already downloaded and verified\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Files already downloaded and verified\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "----------------------------------------------\n",
      "Class: 8 lam: 0.5 # epochs: 300 lr: 0.001\n",
      "**********************************\n",
      "repetition number: 0\n",
      "  Epoch 1/300\t Time: 1.111\t Loss: 69.13986527 \t Test AUC :0.3724 \n",
      "  Epoch 51/300\t Time: 1.016\t Loss: 10.38390712 \t Test AUC :0.3167 \n",
      "  Epoch 101/300\t Time: 1.116\t Loss: 1.69562202 \t Test AUC :0.2584 \n",
      "  Epoch 151/300\t Time: 1.040\t Loss: 0.39419541 \t Test AUC :0.3179 \n",
      "  Epoch 201/300\t Time: 1.052\t Loss: 0.19926362 \t Test AUC :0.6624 \n",
      "  Epoch 251/300\t Time: 1.178\t Loss: 0.16889181 \t Test AUC :0.7464 \n",
      " Final (TOT. EPOCHS 300)::  Loss: 0.16449017 \t   AUC (Test) :0.7464 \n",
      "Files already downloaded and verified\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Files already downloaded and verified\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "----------------------------------------------\n",
      "Class: 9 lam: 0.001 # epochs: 50 lr: 0.001\n",
      "**********************************\n",
      "repetition number: 0\n",
      "  Epoch 1/50\t Time: 1.173\t Loss: 0.58255723 \t Test AUC :0.7194 \n",
      " Final (TOT. EPOCHS 50)::  Loss: 0.13919941 \t   AUC (Test) :0.7194 \n"
     ]
    }
   ],
   "source": [
    "# number of repetitions per class\n",
    "# Results reported in paper are the summary statistics over 10 repetitions (i.e., n_reps = 10)\n",
    "n_reps = 1\n",
    "\n",
    "logging.basicConfig(filename='CIFAR10_DOC.log', level=logging.INFO)\n",
    "\n",
    "# For every class in CIFAR10 dataset\n",
    "for classNum in range(10):\n",
    "    normalclass = classNum\n",
    "    BATCH_SIZE = 256\n",
    "\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "\n",
    "    ###############################################################################################\n",
    "    #                                     Train Data\n",
    "    ###############################################################################################\n",
    "    cifar_train =  torchvision.datasets.CIFAR10(root=\"./cifar/cifar_train\", train=True, download=True, \n",
    "                                              transform=transform)\n",
    "\n",
    "    balanced_batch_sampler = BalancedBatchSampler(cifar_train, 10, n_samples = BATCH_SIZE, class_id = [normalclass])\n",
    "    train_loader = torch.utils.data.DataLoader(cifar_train, batch_sampler=balanced_batch_sampler, pin_memory=True, num_workers=NUM_WORKERS)\n",
    "\n",
    "\n",
    "    ###############################################################################################\n",
    "    #                                     Evaluations on :-\n",
    "    ###############################################################################################\n",
    "    # Test Data\n",
    "    test_dat = torchvision.datasets.CIFAR10('./cifar', train=False, download=True, transform=transform)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_dat, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True, num_workers=NUM_WORKERS)\n",
    "\n",
    "    # Train (used to Evaluate i.e. TP_rate etc.)\n",
    "    balanced_batch_sampler_eval = BalancedBatchSampler(cifar_train, 10, n_samples = BATCH_SIZE, class_id = [normalclass],allSamples=True)\n",
    "    train_loader_eval = torch.utils.data.DataLoader(cifar_train, batch_sampler=balanced_batch_sampler_eval, pin_memory=True, num_workers=NUM_WORKERS)\n",
    "    \n",
    "    # See Paper's Appendix for the Optimal Parameters\n",
    "    # K = 0, lam = 0.5, EPOCH = 300, SGD, lr = 0.005 \n",
    "    # K = 1, lam = 1.0, EPOCH = 300, SGD, lr = 0.001\n",
    "    # K = 2, lam = 0.5, Epoch = 300, SGD, lr = 0.005\n",
    "    # K = 3, lam = 1.0, Epoch = 300, SGD,lr = 0.001\n",
    "    # K = 4, lam = 1.0, Epoch = 300, SGD,lr = 0.001\n",
    "    # K = 5, lam = 0.5, Epoch = 400, SGD,lr = 0.001\n",
    "    # K = 6, lam = 0.5, Epoch = 300, SGD,lr = 0.001\n",
    "    # K = 7, lam = 1.0, Epoch = 300, SGD,lr = 0.001\n",
    "    # K = 8, lam = 0.5, Epoch = 300, SGD,lr = 0.001\n",
    "    # K = 9, lam = 0.001, Epoch = 50, SGD,lr = 0.001\n",
    "\n",
    "    if classNum in [1,3,4,7]:\n",
    "        lam = 1.0\n",
    "    elif classNum == 9:\n",
    "        lam = 0.001\n",
    "    else:\n",
    "        lam = 0.5\n",
    "        \n",
    "    if classNum == 5:\n",
    "        EPOCH = 400\n",
    "    elif classNum == 9:\n",
    "        EPOCH = 50\n",
    "    else:\n",
    "        EPOCH = 300\n",
    "        \n",
    "    if classNum == 0 or classNum == 2:\n",
    "        lr = 0.005\n",
    "    else:\n",
    "        lr = 0.001\n",
    "    \n",
    "    print('----------------------------------------------')\n",
    "    print('Class: '+str(classNum)+' lam: '+str(lam)+' # epochs: '+str(EPOCH)+' lr: '+str(lr))\n",
    "    logging.info('----------------------------------------------')\n",
    "    logging.info('Class: '+str(classNum)+' lam: '+str(lam)+' # epochs: '+str(EPOCH)+' lr: '+str(lr))\n",
    "    \n",
    "    for repetition in range(n_reps):\n",
    "        print('**********************************')\n",
    "        print('repetition number: '+str(repetition))\n",
    "        logging.info('**********************************')\n",
    "        logging.info('repetition number: '+str(repetition))\n",
    "\n",
    "        net = CIFAR10_LeNet()\n",
    "        net = net.to(device)\n",
    "\n",
    "        optimizer = torch.optim.SGD(net.parameters(), lr = lr)  \n",
    "        train_loss = HingeLoss()\n",
    "\n",
    "        for epoch in range(EPOCH):\n",
    "\n",
    "            loss_epoch = 0.0\n",
    "            n_batches = 0\n",
    "            epoch_start_time = time.time()\n",
    "\n",
    "            for (i,data) in enumerate(train_loader):\n",
    "                inputs, labels = data\n",
    "                inputs = inputs.to(device)\n",
    "                ytr = np.ones((len(inputs),1))\n",
    "                ytr = torch.from_numpy(ytr).to(device).float()\n",
    "\n",
    "                # Zero the network parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Update network parameters via backpropagation: forward + backward + optimize\n",
    "                outputs = net(inputs) \n",
    "                lam = torch.tensor(lam).to(device)\n",
    "                l2_reg = torch.tensor(0.).to(device)\n",
    "\n",
    "                for name, param in net.named_parameters():\n",
    "                    l2_reg += torch.norm(param)**2\n",
    "\n",
    "\n",
    "                loss = train_loss(outputs,ytr)\n",
    "                loss += lam * l2_reg\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                loss_epoch += loss.item()\n",
    "                n_batches += 1\n",
    "\n",
    "            if epoch % 50 == 0: # Every 50 Epochs!\n",
    "                idx_label_score = []\n",
    "                net.eval()\n",
    "                with torch.no_grad():\n",
    "\n",
    "                    for data in test_loader:\n",
    "                        inputs, labels = data\n",
    "\n",
    "                        labels = labels.cpu().data.numpy()\n",
    "                        labels = (labels==normalclass).astype(int)\n",
    "\n",
    "                        inputs = inputs.to(device)\n",
    "                        outputs = net(inputs)\n",
    "                        scores = outputs.data.cpu().numpy()\n",
    "                        idx_label_score += list(zip(labels.tolist(),\n",
    "                                                    scores.tolist()))\n",
    "\n",
    "\n",
    "                tstlabels, scores = zip(*idx_label_score)\n",
    "                tstlabels = np.array(tstlabels)\n",
    "                tstlabels[np.where(tstlabels==0)]=-1.0\n",
    "                scores = np.array(scores)\n",
    "                Tst_auc_score = evalmetrics(tstlabels,scores.flatten())   \n",
    "\n",
    "                # log epoch statistics\n",
    "                epoch_train_time = time.time() - epoch_start_time\n",
    "                print('  Epoch {}/{}\\t Time: {:.3f}\\t Loss: {:.8f} \\t Test AUC :{:.4f} '\n",
    "                            .format(epoch + 1, EPOCH, epoch_train_time, loss_epoch / n_batches, Tst_auc_score))\n",
    "                logging.info('  Epoch {}/{}\\t Time: {:.3f}\\t Loss: {:.8f} \\t Test AUC :{:.4f} '\n",
    "                            .format(epoch + 1, EPOCH, epoch_train_time, loss_epoch / n_batches, Tst_auc_score))\n",
    "\n",
    "        # Final Solution\n",
    "        print(' Final (TOT. EPOCHS {})::  Loss: {:.8f} \\t   AUC (Test) :{:.4f} '\n",
    "                .format(EPOCH, loss_epoch / n_batches, Tst_auc_score))\n",
    "        logging.info(' Final (TOT. EPOCHS {})::  Loss: {:.8f} \\t   AUC (Test) :{:.4f} '\n",
    "                .format(EPOCH, loss_epoch / n_batches, Tst_auc_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
