{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep One Class Classification using Contradictions (DOC3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import logging\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import roc_auc_score\n",
    "#from torchvision import datasets, transforms\n",
    "#from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import BatchSampler\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For code optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of workers:  6\n"
     ]
    }
   ],
   "source": [
    "# uncomment for code optimization\n",
    "#torch.backends.cudnn.benchmark = True\n",
    "\n",
    "NUM_WORKERS = int(os.cpu_count() / 2)\n",
    "print('number of workers: ', NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a. Evaluation (Loss, Eval Metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function\n",
    "class HingeLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HingeLoss, self).__init__()\n",
    "    \n",
    "    def forward(self, ypred, ytrue, margin = 1.0, smooth = False):\n",
    "        ypred = ypred.squeeze()\n",
    "        if smooth:\n",
    "            loss = torch.nn.Softplus()\n",
    "            out = torch.mean(loss(margin - (ytrue * ypred)))\n",
    "        else:\n",
    "            out = torch.mean(torch.relu(margin - (ytrue * ypred)))\n",
    "        return out\n",
    "\n",
    "\n",
    "def evalmetrics(y_true, scores):\n",
    "    auc_score = roc_auc_score(y_true, scores)\n",
    "    \n",
    "    return auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b. Data Set Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BalancedBatchSampler(BatchSampler):\n",
    "    \"\"\"\n",
    "    BatchSampler - from a MNIST-like dataset, samples n_classes and within these classes samples n_samples.\n",
    "    Returns batches of size n_classes * n_samples\n",
    "    # CODE SOURCE : https://discuss.pytorch.org/t/load-the-same-number-of-data-per-class/65198/4\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, n_classes, n_samples, class_id = None, allSamples = False):\n",
    "        loader = torch.utils.data.DataLoader(dataset)\n",
    "        self.labels_list = []\n",
    "        \n",
    "        for _, label in loader:\n",
    "            self.labels_list.append(label)\n",
    "        \n",
    "        self.labels = torch.LongTensor(self.labels_list)\n",
    "        self.labels_set = list(set(self.labels.numpy()))\n",
    "        print(self.labels_set)\n",
    "        \n",
    "        self.label_to_indices = {label: np.where(self.labels.numpy() == label)[0]\n",
    "                                 for label in self.labels_set}\n",
    "        \n",
    "        for l in self.labels_set:\n",
    "            np.random.shuffle(self.label_to_indices[l])\n",
    "        self.used_label_indices_count = {label: 0 for label in self.labels_set}\n",
    "        self.count = 0\n",
    "        self.n_classes = n_classes\n",
    "        self.n_samples = n_samples\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = self.n_samples * self.n_classes\n",
    "        self.class_id = class_id\n",
    "        self.allSamples = allSamples\n",
    " \n",
    "    def __iter__(self):\n",
    "        self.count = 0\n",
    "        while self.count + self.batch_size < len(self.dataset):\n",
    "            \n",
    "            if self.class_id is not None:\n",
    "                classes = self.class_id\n",
    "            else:\n",
    "                classes = np.random.choice(self.labels_set, self.n_classes, replace=False)\n",
    "            \n",
    "            indices = []\n",
    "            for class_ in classes:\n",
    "                if not self.allSamples: \n",
    "                    indices.extend(self.label_to_indices[class_][\n",
    "                                   self.used_label_indices_count[class_]:self.used_label_indices_count[\n",
    "                                                                        class_] + self.n_samples])\n",
    "                else: \n",
    "                    indices.extend(self.label_to_indices[class_])\n",
    "                \n",
    "                self.used_label_indices_count[class_] += self.n_samples\n",
    "                \n",
    "                if self.used_label_indices_count[class_] + self.n_samples > len(self.label_to_indices[class_]):\n",
    "                    np.random.shuffle(self.label_to_indices[class_])\n",
    "                    self.used_label_indices_count[class_] = 0\n",
    "                    \n",
    "            yield indices\n",
    "            \n",
    "            self.count += self.n_classes * self.n_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset) // self.batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10_LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CIFAR10_LeNet, self).__init__()\n",
    "\n",
    "        self.rep_dim = 128\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 32, 5, bias=False, padding=2)\n",
    "        self.bn2d1 = nn.BatchNorm2d(32, eps=1e-04, affine=False)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 5, bias=False, padding=2)\n",
    "        self.bn2d2 = nn.BatchNorm2d(64, eps=1e-04, affine=False)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 5, bias=False, padding=2)\n",
    "        self.bn2d3 = nn.BatchNorm2d(128, eps=1e-04, affine=False)\n",
    "        self.fc1 = nn.Linear(128 * 4 * 4, self.rep_dim, bias=False)\n",
    "        self.fc2 = nn.Linear(self.rep_dim, int(self.rep_dim/2), bias=False)\n",
    "        self.fc3 = nn.Linear(int(self.rep_dim/2), 1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool(F.leaky_relu(self.bn2d1(x)))\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool(F.leaky_relu(self.bn2d2(x)))\n",
    "        x = self.conv3(x)\n",
    "        x = self.pool(F.leaky_relu(self.bn2d3(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename='Cifar10_DOC3.log', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of repetitions per class\n",
    "# Results reported in paper are the summary statistics over 10 repetitions (i.e., n_reps = 10)\n",
    "n_reps = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0 Training data: class 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************\n",
      "Normal class: \n",
      "0\n",
      "lam = 0.1,lr = 0.005, Cu = 0.5\n",
      "Exp No: 0\n",
      "Files already downloaded and verified\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Files already downloaded and verified\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  Epoch 0/350\t Time: 2.533\t Loss: 13.92626451 \t Test AUC :0.5304 \n",
      "  Epoch 50/350\t Time: 1.632\t Loss: 2.08640998 \t Test AUC :0.6742 \n",
      "  Epoch 100/350\t Time: 1.544\t Loss: 0.36120845 \t Test AUC :0.7239 \n",
      "  Epoch 150/350\t Time: 1.548\t Loss: 0.10845619 \t Test AUC :0.7994 \n",
      "  Epoch 200/350\t Time: 1.692\t Loss: 0.07070498 \t Test AUC :0.8087 \n",
      "  Epoch 250/350\t Time: 1.630\t Loss: 0.06384046 \t Test AUC :0.8056 \n",
      "  Epoch 300/350\t Time: 1.533\t Loss: 0.06316035 \t Test AUC :0.8099 \n",
      "  Epoch 350/350\t Time: 1.598\t Loss: 0.06313583 \t Test AUC :0.8132 \n",
      " Final (TOT. EPOCHS 350)::  Loss: 0.06313583 \t   AUC (Test) :0.8132 \n"
     ]
    }
   ],
   "source": [
    "EPOCH = 350 \n",
    "BATCH_SIZEU = 256\n",
    "BATCH_SIZE = 256\n",
    "normalclass = 0\n",
    "\n",
    "print('********************************')\n",
    "print('Normal class: ')\n",
    "print(normalclass)\n",
    "logging.info('********************************')\n",
    "logging.info('Normal class: ')\n",
    "logging.info(normalclass)\n",
    "\n",
    "print('lam = {},lr = {}, Cu = {}'.format(0.1,0.005,0.5))\n",
    "logging.info('lam = {},lr = {}, Cu = {}'.format(0.1,0.005,0.5))\n",
    "\n",
    "\n",
    "transform = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "    \n",
    "for exp in range(n_reps):\n",
    "    \n",
    "    print('Exp No: {}'.format(exp))\n",
    "    logging.info('Exp No: {}'.format(exp))\n",
    "\n",
    "    ###############################################################################################\n",
    "    #                                     Train Data\n",
    "    ###############################################################################################\n",
    "    cifar_train =  torchvision.datasets.CIFAR10(root=\"./cifar/cifar_train\", train=True, download=True, \n",
    "                                              transform=transform)\n",
    "\n",
    "    balanced_batch_sampler = BalancedBatchSampler(cifar_train, 10, n_samples = BATCH_SIZE, class_id = [normalclass])\n",
    "    train_loader = torch.utils.data.DataLoader(cifar_train, batch_sampler=balanced_batch_sampler, pin_memory=True, num_workers=NUM_WORKERS)\n",
    "\n",
    "    \n",
    "    ###############################################################################################\n",
    "    #                                     Evaluations on :-\n",
    "    ###############################################################################################\n",
    "    # Test Data\n",
    "    test_dat = torchvision.datasets.CIFAR10('./cifar', train=False, download=True, transform=transform)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_dat, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True, num_workers=NUM_WORKERS)\n",
    "    \n",
    "    \n",
    "    # Train (used to Evaluate i.e. TP_rate etc.)\n",
    "    balanced_batch_sampler_eval = BalancedBatchSampler(cifar_train, 10, n_samples = BATCH_SIZE, class_id = [normalclass],allSamples=True)\n",
    "    train_loader_eval = torch.utils.data.DataLoader(cifar_train, batch_sampler=balanced_batch_sampler_eval, pin_memory=True, num_workers=NUM_WORKERS)\n",
    "\n",
    "    \n",
    "    lam = 0.1\n",
    "    lr = 0.005\n",
    "    Cu = 0.5\n",
    "    \n",
    "    \n",
    "    eps = 0.0\n",
    "    dpos = 1.0-eps\n",
    "    dneg = -(1.0+eps)\n",
    "\n",
    "    net = CIFAR10_LeNet()\n",
    "    net = net.to(device)\n",
    "\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr = lr)  \n",
    "\n",
    "    train_loss = HingeLoss()\n",
    "    unlabeled_posloss = HingeLoss()\n",
    "    unlabeled_negloss = HingeLoss()\n",
    "\n",
    "    for epoch in range(EPOCH+1):\n",
    "\n",
    "        loss_epoch = 0.0\n",
    "        n_batches = 0\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        for (i,data) in enumerate(train_loader):\n",
    "\n",
    "            # Training Data\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            ytr = np.ones(len(inputs))\n",
    "            ytr = torch.from_numpy(ytr).to(device).float()\n",
    "            outputs = net(inputs) \n",
    "\n",
    "            XU = torch.from_numpy(np.random.randn(BATCH_SIZEU, 3, 32, 32))\n",
    "\n",
    "            XU = XU.to(device).float()\n",
    "            outputsU = net(XU)\n",
    "            yunppos = np.ones(BATCH_SIZE)\n",
    "            yunpneg = -np.ones(BATCH_SIZE)\n",
    "            yupos = torch.from_numpy(yunppos).to(device).float()\n",
    "            yuneg = torch.from_numpy(yunpneg).to(device).float()\n",
    "\n",
    "            \n",
    "            # Zero the network parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Loss\n",
    "            loss = train_loss(outputs,ytr,smooth = False)\n",
    "\n",
    "            # Unlabeled data\n",
    "            lossunlab = unlabeled_posloss(outputsU,yupos,dpos,smooth = False) + unlabeled_negloss(outputsU,yuneg,dneg,smooth = False)\n",
    "            loss+=Cu*lossunlab\n",
    "\n",
    "            lam = torch.tensor(lam).to(device)\n",
    "            l2_reg = torch.tensor(0.).to(device)\n",
    "\n",
    "            for name, param in net.named_parameters():\n",
    "                l2_reg += torch.norm(param)**2\n",
    "\n",
    "            loss += lam * l2_reg\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_epoch += loss.item()\n",
    "            n_batches += 1\n",
    "\n",
    "\n",
    "        if epoch%50==0:\n",
    "\n",
    "            idx_label_score = []\n",
    "            net.eval()\n",
    "            with torch.no_grad():\n",
    "                for data in test_loader:\n",
    "                    inputs, labels = data\n",
    "\n",
    "                    labels = labels.cpu().data.numpy()\n",
    "                    labels = (labels==normalclass).astype(int)\n",
    "\n",
    "                    inputs = inputs.to(device)\n",
    "                    outputs = net(inputs)\n",
    "                    scores = outputs.data.cpu().numpy()\n",
    "                    idx_label_score += list(zip(labels.tolist(),\n",
    "                                                scores.tolist()))\n",
    "\n",
    "            tstlabels, scores = zip(*idx_label_score)\n",
    "            tstlabels = np.array(tstlabels)\n",
    "\n",
    "            tstlabels[np.where(tstlabels==0)]=-1.0\n",
    "            scores = np.array(scores)\n",
    "\n",
    "            Tst_auc_score = evalmetrics(tstlabels,scores.flatten())\n",
    "\n",
    "            # log epoch statistics\n",
    "            epoch_train_time = time.time() - epoch_start_time\n",
    "            logging.info('  Epoch {}/{}\\t Time: {:.3f}\\t Loss: {:.8f} \\t Test AUC :{:.4f} '\n",
    "                        .format(epoch , EPOCH, epoch_train_time, loss_epoch / n_batches, Tst_auc_score))\n",
    "\n",
    "            print('  Epoch {}/{}\\t Time: {:.3f}\\t Loss: {:.8f} \\t Test AUC :{:.4f} '\n",
    "                        .format(epoch , EPOCH, epoch_train_time, loss_epoch / n_batches, Tst_auc_score))\n",
    "\n",
    "\n",
    "    # Final Solution\n",
    "    logging.info(' Final (TOT. EPOCHS {})::  Loss: {:.8f} \\t   AUC (Test) :{:.4f} '\n",
    "            .format(EPOCH, loss_epoch / n_batches, Tst_auc_score))\n",
    "\n",
    "    print(' Final (TOT. EPOCHS {})::  Loss: {:.8f} \\t   AUC (Test) :{:.4f} '\n",
    "            .format(EPOCH, loss_epoch / n_batches, Tst_auc_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Training data: class 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************\n",
      "Normal class: \n",
      "1\n",
      "lam = 0.05,lr = 0.0005, Cu = 0.1\n",
      "Exp No: 0\n",
      "Files already downloaded and verified\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Files already downloaded and verified\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  Epoch 0/350\t Time: 1.632\t Loss: 7.70551486 \t Test AUC :0.6038 \n",
      "  Epoch 50/350\t Time: 1.698\t Loss: 6.30869429 \t Test AUC :0.7135 \n",
      "  Epoch 100/350\t Time: 1.676\t Loss: 5.73662268 \t Test AUC :0.7233 \n",
      "  Epoch 150/350\t Time: 1.651\t Loss: 5.21678822 \t Test AUC :0.7278 \n",
      "  Epoch 200/350\t Time: 1.725\t Loss: 4.74436348 \t Test AUC :0.7270 \n",
      "  Epoch 250/350\t Time: 1.611\t Loss: 4.31490803 \t Test AUC :0.7290 \n",
      "  Epoch 300/350\t Time: 1.609\t Loss: 3.92443804 \t Test AUC :0.7320 \n",
      "  Epoch 350/350\t Time: 1.643\t Loss: 3.56951341 \t Test AUC :0.7344 \n",
      " Final (TOT. EPOCHS 350)::  Loss: 3.56951341 \t   AUC (Test) :0.7344 \n"
     ]
    }
   ],
   "source": [
    "EPOCH = 350 \n",
    "BATCH_SIZEU = 256\n",
    "BATCH_SIZE = 256\n",
    "normalclass = 1\n",
    "\n",
    "print('********************************')\n",
    "print('Normal class: ')\n",
    "print(normalclass)\n",
    "logging.info('********************************')\n",
    "logging.info('Normal class: ')\n",
    "logging.info(normalclass)\n",
    "\n",
    "print('lam = {},lr = {}, Cu = {}'.format(0.05,0.0005,0.1))\n",
    "logging.info('lam = {},lr = {}, Cu = {}'.format(0.05,0.0005,0.1))\n",
    "\n",
    "\n",
    "transform = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "    \n",
    "for exp in range(n_reps):\n",
    "    \n",
    "    print('Exp No: {}'.format(exp))\n",
    "    logging.info('Exp No: {}'.format(exp))\n",
    "\n",
    "    ###############################################################################################\n",
    "    #                                     Train Data\n",
    "    ###############################################################################################\n",
    "    cifar_train =  torchvision.datasets.CIFAR10(root=\"./cifar/cifar_train\", train=True, download=True, \n",
    "                                              transform=transform)\n",
    "\n",
    "    balanced_batch_sampler = BalancedBatchSampler(cifar_train, 10, n_samples = BATCH_SIZE, class_id = [normalclass])\n",
    "    train_loader = torch.utils.data.DataLoader(cifar_train, batch_sampler=balanced_batch_sampler, pin_memory=True, num_workers=NUM_WORKERS)\n",
    "\n",
    "    \n",
    "    ###############################################################################################\n",
    "    #                                     Evaluations on :-\n",
    "    ###############################################################################################\n",
    "    # Test Data\n",
    "    test_dat = torchvision.datasets.CIFAR10('./cifar', train=False, download=True, transform=transform)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_dat, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True, num_workers=NUM_WORKERS)\n",
    "    \n",
    "    \n",
    "    # Train (used to Evaluate i.e. TP_rate etc.)\n",
    "    balanced_batch_sampler_eval = BalancedBatchSampler(cifar_train, 10, n_samples = BATCH_SIZE, class_id = [normalclass],allSamples=True)\n",
    "    train_loader_eval = torch.utils.data.DataLoader(cifar_train, batch_sampler=balanced_batch_sampler_eval, pin_memory=True, num_workers=NUM_WORKERS)\n",
    "\n",
    "    \n",
    "    lam = 0.05\n",
    "    lr = 0.0005\n",
    "    Cu = 0.1\n",
    "    \n",
    "    \n",
    "    eps = 0.0\n",
    "    dpos = 1.0-eps\n",
    "    dneg = -(1.0+eps)\n",
    "\n",
    "    net = CIFAR10_LeNet()\n",
    "    net = net.to(device)\n",
    "\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr = lr)  \n",
    "\n",
    "    train_loss = HingeLoss()\n",
    "    unlabeled_posloss = HingeLoss()\n",
    "    unlabeled_negloss = HingeLoss()\n",
    "\n",
    "    for epoch in range(EPOCH+1):\n",
    "\n",
    "        loss_epoch = 0.0\n",
    "        n_batches = 0\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        for (i,data) in enumerate(train_loader):\n",
    "\n",
    "            # Training Data\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            ytr = np.ones(len(inputs))\n",
    "            ytr = torch.from_numpy(ytr).to(device).float()\n",
    "            outputs = net(inputs) \n",
    "\n",
    "            XU = torch.from_numpy(np.random.randn(BATCH_SIZEU, 3, 32, 32))\n",
    "\n",
    "            XU = XU.to(device).float()\n",
    "            outputsU = net(XU)\n",
    "            yunppos = np.ones(BATCH_SIZE)\n",
    "            yunpneg = -np.ones(BATCH_SIZE)\n",
    "            yupos = torch.from_numpy(yunppos).to(device).float()\n",
    "            yuneg = torch.from_numpy(yunpneg).to(device).float()\n",
    "\n",
    "            \n",
    "            # Zero the network parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Loss\n",
    "            loss = train_loss(outputs,ytr,smooth = False)\n",
    "\n",
    "            # Unlabeled data\n",
    "            lossunlab = unlabeled_posloss(outputsU,yupos,dpos,smooth = False) + unlabeled_negloss(outputsU,yuneg,dneg,smooth = False)\n",
    "            loss+=Cu*lossunlab\n",
    "\n",
    "            lam = torch.tensor(lam).to(device)\n",
    "            l2_reg = torch.tensor(0.).to(device)\n",
    "\n",
    "            for name, param in net.named_parameters():\n",
    "                l2_reg += torch.norm(param)**2\n",
    "\n",
    "            loss += lam * l2_reg\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_epoch += loss.item()\n",
    "            n_batches += 1\n",
    "\n",
    "\n",
    "        if epoch%50==0:\n",
    "\n",
    "            idx_label_score = []\n",
    "            net.eval()\n",
    "            with torch.no_grad():\n",
    "                for data in test_loader:\n",
    "                    inputs, labels = data\n",
    "\n",
    "                    labels = labels.cpu().data.numpy()\n",
    "                    labels = (labels==normalclass).astype(int)\n",
    "\n",
    "                    inputs = inputs.to(device)\n",
    "                    outputs = net(inputs)\n",
    "                    scores = outputs.data.cpu().numpy()\n",
    "                    idx_label_score += list(zip(labels.tolist(),\n",
    "                                                scores.tolist()))\n",
    "\n",
    "            tstlabels, scores = zip(*idx_label_score)\n",
    "            tstlabels = np.array(tstlabels)\n",
    "\n",
    "            tstlabels[np.where(tstlabels==0)]=-1.0\n",
    "            scores = np.array(scores)\n",
    "\n",
    "            Tst_auc_score = evalmetrics(tstlabels,scores.flatten())\n",
    "\n",
    "            # log epoch statistics\n",
    "            epoch_train_time = time.time() - epoch_start_time\n",
    "            logging.info('  Epoch {}/{}\\t Time: {:.3f}\\t Loss: {:.8f} \\t Test AUC :{:.4f} '\n",
    "                        .format(epoch , EPOCH, epoch_train_time, loss_epoch / n_batches, Tst_auc_score))\n",
    "\n",
    "            print('  Epoch {}/{}\\t Time: {:.3f}\\t Loss: {:.8f} \\t Test AUC :{:.4f} '\n",
    "                        .format(epoch , EPOCH, epoch_train_time, loss_epoch / n_batches, Tst_auc_score))\n",
    "\n",
    "\n",
    "    # Final Solution\n",
    "    logging.info(' Final (TOT. EPOCHS {})::  Loss: {:.8f} \\t   AUC (Test) :{:.4f} '\n",
    "            .format(EPOCH, loss_epoch / n_batches, Tst_auc_score))\n",
    "\n",
    "    print(' Final (TOT. EPOCHS {})::  Loss: {:.8f} \\t   AUC (Test) :{:.4f} '\n",
    "            .format(EPOCH, loss_epoch / n_batches, Tst_auc_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Training data: class 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************\n",
      "Normal class: \n",
      "2\n",
      "lam = 0.05,lr = 0.005, Cu = 0.5\n",
      "Exp No: 0\n",
      "Files already downloaded and verified\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Files already downloaded and verified\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  Epoch 0/350\t Time: 1.785\t Loss: 7.12907111 \t Test AUC :0.5095 \n",
      "  Epoch 50/350\t Time: 1.694\t Loss: 2.68724964 \t Test AUC :0.6051 \n",
      "  Epoch 100/350\t Time: 1.685\t Loss: 1.06182805 \t Test AUC :0.6378 \n",
      "  Epoch 150/350\t Time: 1.648\t Loss: 0.43700445 \t Test AUC :0.6418 \n",
      "  Epoch 200/350\t Time: 1.729\t Loss: 0.19621352 \t Test AUC :0.6544 \n",
      "  Epoch 250/350\t Time: 1.669\t Loss: 0.10839283 \t Test AUC :0.6820 \n",
      "  Epoch 300/350\t Time: 1.675\t Loss: 0.06906865 \t Test AUC :0.6881 \n",
      "  Epoch 350/350\t Time: 1.688\t Loss: 0.05747793 \t Test AUC :0.6870 \n",
      " Final (TOT. EPOCHS 350)::  Loss: 0.05747793 \t   AUC (Test) :0.6870 \n"
     ]
    }
   ],
   "source": [
    "EPOCH = 350 \n",
    "BATCH_SIZEU = 256\n",
    "BATCH_SIZE = 256\n",
    "normalclass = 2\n",
    "\n",
    "print('********************************')\n",
    "print('Normal class: ')\n",
    "print(normalclass)\n",
    "logging.info('********************************')\n",
    "logging.info('Normal class: ')\n",
    "logging.info(normalclass)\n",
    "\n",
    "print('lam = {},lr = {}, Cu = {}'.format(0.05,0.005,0.5))\n",
    "logging.info('lam = {},lr = {}, Cu = {}'.format(0.05,0.005,0.5))\n",
    "\n",
    "\n",
    "transform = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "    \n",
    "for exp in range(n_reps):\n",
    "    \n",
    "    print('Exp No: {}'.format(exp))\n",
    "    logging.info('Exp No: {}'.format(exp))\n",
    "\n",
    "    ###############################################################################################\n",
    "    #                                     Train Data\n",
    "    ###############################################################################################\n",
    "    cifar_train =  torchvision.datasets.CIFAR10(root=\"./cifar/cifar_train\", train=True, download=True, \n",
    "                                              transform=transform)\n",
    "\n",
    "    balanced_batch_sampler = BalancedBatchSampler(cifar_train, 10, n_samples = BATCH_SIZE, class_id = [normalclass])\n",
    "    train_loader = torch.utils.data.DataLoader(cifar_train, batch_sampler=balanced_batch_sampler, pin_memory=True, num_workers=NUM_WORKERS)\n",
    "\n",
    "    \n",
    "    ###############################################################################################\n",
    "    #                                     Evaluations on :-\n",
    "    ###############################################################################################\n",
    "    # Test Data\n",
    "    test_dat = torchvision.datasets.CIFAR10('./cifar', train=False, download=True, transform=transform)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_dat, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True, num_workers=NUM_WORKERS)\n",
    "    \n",
    "    \n",
    "    # Train (used to Evaluate i.e. TP_rate etc.)\n",
    "    balanced_batch_sampler_eval = BalancedBatchSampler(cifar_train, 10, n_samples = BATCH_SIZE, class_id = [normalclass],allSamples=True)\n",
    "    train_loader_eval = torch.utils.data.DataLoader(cifar_train, batch_sampler=balanced_batch_sampler_eval, pin_memory=True, num_workers=NUM_WORKERS)\n",
    "\n",
    "    \n",
    "    lam = 0.05\n",
    "    lr = 0.005\n",
    "    Cu = 0.5\n",
    "    \n",
    "    \n",
    "    eps = 0.0\n",
    "    dpos = 1.0-eps\n",
    "    dneg = -(1.0+eps)\n",
    "\n",
    "    net = CIFAR10_LeNet()\n",
    "    net = net.to(device)\n",
    "\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr = lr)  \n",
    "\n",
    "    train_loss = HingeLoss()\n",
    "    unlabeled_posloss = HingeLoss()\n",
    "    unlabeled_negloss = HingeLoss()\n",
    "\n",
    "    for epoch in range(EPOCH+1):\n",
    "\n",
    "        loss_epoch = 0.0\n",
    "        n_batches = 0\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        for (i,data) in enumerate(train_loader):\n",
    "\n",
    "            # Training Data\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            ytr = np.ones(len(inputs))\n",
    "            ytr = torch.from_numpy(ytr).to(device).float()\n",
    "            outputs = net(inputs) \n",
    "\n",
    "            XU = torch.from_numpy(np.random.randn(BATCH_SIZEU, 3, 32, 32))\n",
    "\n",
    "            XU = XU.to(device).float()\n",
    "            outputsU = net(XU)\n",
    "            yunppos = np.ones(BATCH_SIZE)\n",
    "            yunpneg = -np.ones(BATCH_SIZE)\n",
    "            yupos = torch.from_numpy(yunppos).to(device).float()\n",
    "            yuneg = torch.from_numpy(yunpneg).to(device).float()\n",
    "\n",
    "            \n",
    "            # Zero the network parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Loss\n",
    "            loss = train_loss(outputs,ytr,smooth = False)\n",
    "\n",
    "            # Unlabeled data\n",
    "            lossunlab = unlabeled_posloss(outputsU,yupos,dpos,smooth = False) + unlabeled_negloss(outputsU,yuneg,dneg,smooth = False)\n",
    "            loss+=Cu*lossunlab\n",
    "\n",
    "            lam = torch.tensor(lam).to(device)\n",
    "            l2_reg = torch.tensor(0.).to(device)\n",
    "\n",
    "            for name, param in net.named_parameters():\n",
    "                l2_reg += torch.norm(param)**2\n",
    "\n",
    "            loss += lam * l2_reg\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_epoch += loss.item()\n",
    "            n_batches += 1\n",
    "\n",
    "\n",
    "        if epoch%50==0:\n",
    "\n",
    "            idx_label_score = []\n",
    "            net.eval()\n",
    "            with torch.no_grad():\n",
    "                for data in test_loader:\n",
    "                    inputs, labels = data\n",
    "\n",
    "                    labels = labels.cpu().data.numpy()\n",
    "                    labels = (labels==normalclass).astype(int)\n",
    "\n",
    "                    inputs = inputs.to(device)\n",
    "                    outputs = net(inputs)\n",
    "                    scores = outputs.data.cpu().numpy()\n",
    "                    idx_label_score += list(zip(labels.tolist(),\n",
    "                                                scores.tolist()))\n",
    "\n",
    "            tstlabels, scores = zip(*idx_label_score)\n",
    "            tstlabels = np.array(tstlabels)\n",
    "\n",
    "            tstlabels[np.where(tstlabels==0)]=-1.0\n",
    "            scores = np.array(scores)\n",
    "\n",
    "            Tst_auc_score = evalmetrics(tstlabels,scores.flatten())\n",
    "\n",
    "            # log epoch statistics\n",
    "            epoch_train_time = time.time() - epoch_start_time\n",
    "            logging.info('  Epoch {}/{}\\t Time: {:.3f}\\t Loss: {:.8f} \\t Test AUC :{:.4f} '\n",
    "                        .format(epoch , EPOCH, epoch_train_time, loss_epoch / n_batches, Tst_auc_score))\n",
    "\n",
    "            print('  Epoch {}/{}\\t Time: {:.3f}\\t Loss: {:.8f} \\t Test AUC :{:.4f} '\n",
    "                        .format(epoch , EPOCH, epoch_train_time, loss_epoch / n_batches, Tst_auc_score))\n",
    "\n",
    "\n",
    "    # Final Solution\n",
    "    logging.info(' Final (TOT. EPOCHS {})::  Loss: {:.8f} \\t   AUC (Test) :{:.4f} '\n",
    "            .format(EPOCH, loss_epoch / n_batches, Tst_auc_score))\n",
    "\n",
    "    print(' Final (TOT. EPOCHS {})::  Loss: {:.8f} \\t   AUC (Test) :{:.4f} '\n",
    "            .format(EPOCH, loss_epoch / n_batches, Tst_auc_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Training data: class 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************\n",
      "Normal class: \n",
      "3\n",
      "lam = 0.1,lr = 0.005, Cu = 1.0\n",
      "Exp No: 0\n",
      "Files already downloaded and verified\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Files already downloaded and verified\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  Epoch 0/350\t Time: 1.842\t Loss: 14.13794849 \t Test AUC :0.4979 \n",
      "  Epoch 50/350\t Time: 1.765\t Loss: 2.14326912 \t Test AUC :0.5305 \n",
      "  Epoch 100/350\t Time: 1.718\t Loss: 0.43353922 \t Test AUC :0.5834 \n",
      "  Epoch 150/350\t Time: 1.798\t Loss: 0.18824741 \t Test AUC :0.6255 \n",
      "  Epoch 200/350\t Time: 1.700\t Loss: 0.15213570 \t Test AUC :0.6284 \n",
      "  Epoch 250/350\t Time: 1.689\t Loss: 0.14192049 \t Test AUC :0.6278 \n",
      "  Epoch 300/350\t Time: 1.655\t Loss: 0.14947615 \t Test AUC :0.6263 \n",
      "  Epoch 350/350\t Time: 1.798\t Loss: 0.14925644 \t Test AUC :0.6247 \n",
      " Final (TOT. EPOCHS 350)::  Loss: 0.14925644 \t   AUC (Test) :0.6247 \n"
     ]
    }
   ],
   "source": [
    "EPOCH = 350 \n",
    "BATCH_SIZEU = 256\n",
    "BATCH_SIZE = 256\n",
    "normalclass = 3\n",
    "\n",
    "print('********************************')\n",
    "print('Normal class: ')\n",
    "print(normalclass)\n",
    "logging.info('********************************')\n",
    "logging.info('Normal class: ')\n",
    "logging.info(normalclass)\n",
    "\n",
    "print('lam = {},lr = {}, Cu = {}'.format(0.1,0.005,1.0))\n",
    "logging.info('lam = {},lr = {}, Cu = {}'.format(0.1,0.005,1.0))\n",
    "\n",
    "\n",
    "transform = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "    \n",
    "for exp in range(n_reps):\n",
    "    \n",
    "    print('Exp No: {}'.format(exp))\n",
    "    logging.info('Exp No: {}'.format(exp))\n",
    "\n",
    "    ###############################################################################################\n",
    "    #                                     Train Data\n",
    "    ###############################################################################################\n",
    "    cifar_train =  torchvision.datasets.CIFAR10(root=\"./cifar/cifar_train\", train=True, download=True, \n",
    "                                              transform=transform)\n",
    "\n",
    "    balanced_batch_sampler = BalancedBatchSampler(cifar_train, 10, n_samples = BATCH_SIZE, class_id = [normalclass])\n",
    "    train_loader = torch.utils.data.DataLoader(cifar_train, batch_sampler=balanced_batch_sampler, pin_memory=True, num_workers=NUM_WORKERS)\n",
    "\n",
    "    \n",
    "    ###############################################################################################\n",
    "    #                                     Evaluations on :-\n",
    "    ###############################################################################################\n",
    "    # Test Data\n",
    "    test_dat = torchvision.datasets.CIFAR10('./cifar', train=False, download=True, transform=transform)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_dat, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True, num_workers=NUM_WORKERS)\n",
    "    \n",
    "    \n",
    "    # Train (used to Evaluate i.e. TP_rate etc.)\n",
    "    balanced_batch_sampler_eval = BalancedBatchSampler(cifar_train, 10, n_samples = BATCH_SIZE, class_id = [normalclass],allSamples=True)\n",
    "    train_loader_eval = torch.utils.data.DataLoader(cifar_train, batch_sampler=balanced_batch_sampler_eval, pin_memory=True, num_workers=NUM_WORKERS)\n",
    "\n",
    "    \n",
    "    lam = 0.1\n",
    "    lr = 0.005\n",
    "    Cu = 1.0\n",
    "    \n",
    "    \n",
    "    eps = 0.0\n",
    "    dpos = 1.0-eps\n",
    "    dneg = -(1.0+eps)\n",
    "\n",
    "    net = CIFAR10_LeNet()\n",
    "    net = net.to(device)\n",
    "\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr = lr)  \n",
    "\n",
    "    train_loss = HingeLoss()\n",
    "    unlabeled_posloss = HingeLoss()\n",
    "    unlabeled_negloss = HingeLoss()\n",
    "\n",
    "    for epoch in range(EPOCH+1):\n",
    "\n",
    "        loss_epoch = 0.0\n",
    "        n_batches = 0\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        for (i,data) in enumerate(train_loader):\n",
    "\n",
    "            # Training Data\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            ytr = np.ones(len(inputs))\n",
    "            ytr = torch.from_numpy(ytr).to(device).float()\n",
    "            outputs = net(inputs) \n",
    "\n",
    "            XU = torch.from_numpy(np.random.randn(BATCH_SIZEU, 3, 32, 32))\n",
    "\n",
    "            XU = XU.to(device).float()\n",
    "            outputsU = net(XU)\n",
    "            yunppos = np.ones(BATCH_SIZE)\n",
    "            yunpneg = -np.ones(BATCH_SIZE)\n",
    "            yupos = torch.from_numpy(yunppos).to(device).float()\n",
    "            yuneg = torch.from_numpy(yunpneg).to(device).float()\n",
    "\n",
    "            \n",
    "            # Zero the network parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Loss\n",
    "            loss = train_loss(outputs,ytr,smooth = False)\n",
    "\n",
    "            # Unlabeled data\n",
    "            lossunlab = unlabeled_posloss(outputsU,yupos,dpos,smooth = False) + unlabeled_negloss(outputsU,yuneg,dneg,smooth = False)\n",
    "            loss+=Cu*lossunlab\n",
    "\n",
    "            lam = torch.tensor(lam).to(device)\n",
    "            l2_reg = torch.tensor(0.).to(device)\n",
    "\n",
    "            for name, param in net.named_parameters():\n",
    "                l2_reg += torch.norm(param)**2\n",
    "\n",
    "            loss += lam * l2_reg\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_epoch += loss.item()\n",
    "            n_batches += 1\n",
    "\n",
    "\n",
    "        if epoch%50==0:\n",
    "\n",
    "            idx_label_score = []\n",
    "            net.eval()\n",
    "            with torch.no_grad():\n",
    "                for data in test_loader:\n",
    "                    inputs, labels = data\n",
    "\n",
    "                    labels = labels.cpu().data.numpy()\n",
    "                    labels = (labels==normalclass).astype(int)\n",
    "\n",
    "                    inputs = inputs.to(device)\n",
    "                    outputs = net(inputs)\n",
    "                    scores = outputs.data.cpu().numpy()\n",
    "                    idx_label_score += list(zip(labels.tolist(),\n",
    "                                                scores.tolist()))\n",
    "\n",
    "            tstlabels, scores = zip(*idx_label_score)\n",
    "            tstlabels = np.array(tstlabels)\n",
    "\n",
    "            tstlabels[np.where(tstlabels==0)]=-1.0\n",
    "            scores = np.array(scores)\n",
    "\n",
    "            Tst_auc_score = evalmetrics(tstlabels,scores.flatten())\n",
    "\n",
    "            # log epoch statistics\n",
    "            epoch_train_time = time.time() - epoch_start_time\n",
    "            logging.info('  Epoch {}/{}\\t Time: {:.3f}\\t Loss: {:.8f} \\t Test AUC :{:.4f} '\n",
    "                        .format(epoch , EPOCH, epoch_train_time, loss_epoch / n_batches, Tst_auc_score))\n",
    "\n",
    "            print('  Epoch {}/{}\\t Time: {:.3f}\\t Loss: {:.8f} \\t Test AUC :{:.4f} '\n",
    "                        .format(epoch , EPOCH, epoch_train_time, loss_epoch / n_batches, Tst_auc_score))\n",
    "\n",
    "\n",
    "    # Final Solution\n",
    "    logging.info(' Final (TOT. EPOCHS {})::  Loss: {:.8f} \\t   AUC (Test) :{:.4f} '\n",
    "            .format(EPOCH, loss_epoch / n_batches, Tst_auc_score))\n",
    "\n",
    "    print(' Final (TOT. EPOCHS {})::  Loss: {:.8f} \\t   AUC (Test) :{:.4f} '\n",
    "            .format(EPOCH, loss_epoch / n_batches, Tst_auc_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Training data: class 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************\n",
      "Normal class: \n",
      "4\n",
      "lam = 0.1,lr = 0.001, Cu = 1.0\n",
      "Exp No: 0\n",
      "Files already downloaded and verified\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Files already downloaded and verified\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  Epoch 0/350\t Time: 1.705\t Loss: 14.61869782 \t Test AUC :0.3067 \n",
      "  Epoch 50/350\t Time: 1.643\t Loss: 9.45128857 \t Test AUC :0.4861 \n",
      "  Epoch 100/350\t Time: 1.632\t Loss: 6.47634697 \t Test AUC :0.5483 \n",
      "  Epoch 150/350\t Time: 1.779\t Loss: 4.44396313 \t Test AUC :0.6136 \n",
      "  Epoch 200/350\t Time: 1.646\t Loss: 3.05551900 \t Test AUC :0.6659 \n",
      "  Epoch 250/350\t Time: 1.670\t Loss: 2.10612060 \t Test AUC :0.6978 \n",
      "  Epoch 300/350\t Time: 1.655\t Loss: 1.45633841 \t Test AUC :0.7161 \n",
      "  Epoch 350/350\t Time: 1.746\t Loss: 1.01245551 \t Test AUC :0.7271 \n",
      " Final (TOT. EPOCHS 350)::  Loss: 1.01245551 \t   AUC (Test) :0.7271 \n"
     ]
    }
   ],
   "source": [
    "EPOCH = 350 \n",
    "BATCH_SIZEU = 256\n",
    "BATCH_SIZE = 256\n",
    "normalclass = 4\n",
    "\n",
    "print('********************************')\n",
    "print('Normal class: ')\n",
    "print(normalclass)\n",
    "logging.info('********************************')\n",
    "logging.info('Normal class: ')\n",
    "logging.info(normalclass)\n",
    "\n",
    "print('lam = {},lr = {}, Cu = {}'.format(0.1,0.001,1.0))\n",
    "logging.info('lam = {},lr = {}, Cu = {}'.format(0.1,0.001,1.0))\n",
    "\n",
    "\n",
    "transform = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "    \n",
    "for exp in range(n_reps):\n",
    "    \n",
    "    print('Exp No: {}'.format(exp))\n",
    "    logging.info('Exp No: {}'.format(exp))\n",
    "\n",
    "    ###############################################################################################\n",
    "    #                                     Train Data\n",
    "    ###############################################################################################\n",
    "    cifar_train =  torchvision.datasets.CIFAR10(root=\"./cifar/cifar_train\", train=True, download=True, \n",
    "                                              transform=transform)\n",
    "\n",
    "    balanced_batch_sampler = BalancedBatchSampler(cifar_train, 10, n_samples = BATCH_SIZE, class_id = [normalclass])\n",
    "    train_loader = torch.utils.data.DataLoader(cifar_train, batch_sampler=balanced_batch_sampler, pin_memory=True, num_workers=NUM_WORKERS)\n",
    "\n",
    "    \n",
    "    ###############################################################################################\n",
    "    #                                     Evaluations on :-\n",
    "    ###############################################################################################\n",
    "    # Test Data\n",
    "    test_dat = torchvision.datasets.CIFAR10('./cifar', train=False, download=True, transform=transform)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_dat, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True, num_workers=NUM_WORKERS)\n",
    "    \n",
    "    \n",
    "    # Train (used to Evaluate i.e. TP_rate etc.)\n",
    "    balanced_batch_sampler_eval = BalancedBatchSampler(cifar_train, 10, n_samples = BATCH_SIZE, class_id = [normalclass],allSamples=True)\n",
    "    train_loader_eval = torch.utils.data.DataLoader(cifar_train, batch_sampler=balanced_batch_sampler_eval, pin_memory=True, num_workers=NUM_WORKERS)\n",
    "\n",
    "    \n",
    "    lam = 0.1\n",
    "    lr = 0.001\n",
    "    Cu = 1.0\n",
    "    \n",
    "    \n",
    "    eps = 0.0\n",
    "    dpos = 1.0-eps\n",
    "    dneg = -(1.0+eps)\n",
    "\n",
    "    net = CIFAR10_LeNet()\n",
    "    net = net.to(device)\n",
    "\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr = lr)  \n",
    "\n",
    "    train_loss = HingeLoss()\n",
    "    unlabeled_posloss = HingeLoss()\n",
    "    unlabeled_negloss = HingeLoss()\n",
    "\n",
    "    for epoch in range(EPOCH+1):\n",
    "\n",
    "        loss_epoch = 0.0\n",
    "        n_batches = 0\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        for (i,data) in enumerate(train_loader):\n",
    "\n",
    "            # Training Data\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            ytr = np.ones(len(inputs))\n",
    "            ytr = torch.from_numpy(ytr).to(device).float()\n",
    "            outputs = net(inputs) \n",
    "\n",
    "            XU = torch.from_numpy(np.random.randn(BATCH_SIZEU, 3, 32, 32))\n",
    "\n",
    "            XU = XU.to(device).float()\n",
    "            outputsU = net(XU)\n",
    "            yunppos = np.ones(BATCH_SIZE)\n",
    "            yunpneg = -np.ones(BATCH_SIZE)\n",
    "            yupos = torch.from_numpy(yunppos).to(device).float()\n",
    "            yuneg = torch.from_numpy(yunpneg).to(device).float()\n",
    "\n",
    "            \n",
    "            # Zero the network parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Loss\n",
    "            loss = train_loss(outputs,ytr,smooth = False)\n",
    "\n",
    "            # Unlabeled data\n",
    "            lossunlab = unlabeled_posloss(outputsU,yupos,dpos,smooth = False) + unlabeled_negloss(outputsU,yuneg,dneg,smooth = False)\n",
    "            loss+=Cu*lossunlab\n",
    "\n",
    "            lam = torch.tensor(lam).to(device)\n",
    "            l2_reg = torch.tensor(0.).to(device)\n",
    "\n",
    "            for name, param in net.named_parameters():\n",
    "                l2_reg += torch.norm(param)**2\n",
    "\n",
    "            loss += lam * l2_reg\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_epoch += loss.item()\n",
    "            n_batches += 1\n",
    "\n",
    "\n",
    "        if epoch%50==0:\n",
    "\n",
    "            idx_label_score = []\n",
    "            net.eval()\n",
    "            with torch.no_grad():\n",
    "                for data in test_loader:\n",
    "                    inputs, labels = data\n",
    "\n",
    "                    labels = labels.cpu().data.numpy()\n",
    "                    labels = (labels==normalclass).astype(int)\n",
    "\n",
    "                    inputs = inputs.to(device)\n",
    "                    outputs = net(inputs)\n",
    "                    scores = outputs.data.cpu().numpy()\n",
    "                    idx_label_score += list(zip(labels.tolist(),\n",
    "                                                scores.tolist()))\n",
    "\n",
    "            tstlabels, scores = zip(*idx_label_score)\n",
    "            tstlabels = np.array(tstlabels)\n",
    "\n",
    "            tstlabels[np.where(tstlabels==0)]=-1.0\n",
    "            scores = np.array(scores)\n",
    "\n",
    "            Tst_auc_score = evalmetrics(tstlabels,scores.flatten())\n",
    "\n",
    "            # log epoch statistics\n",
    "            epoch_train_time = time.time() - epoch_start_time\n",
    "            logging.info('  Epoch {}/{}\\t Time: {:.3f}\\t Loss: {:.8f} \\t Test AUC :{:.4f} '\n",
    "                        .format(epoch , EPOCH, epoch_train_time, loss_epoch / n_batches, Tst_auc_score))\n",
    "\n",
    "            print('  Epoch {}/{}\\t Time: {:.3f}\\t Loss: {:.8f} \\t Test AUC :{:.4f} '\n",
    "                        .format(epoch , EPOCH, epoch_train_time, loss_epoch / n_batches, Tst_auc_score))\n",
    "\n",
    "\n",
    "    # Final Solution\n",
    "    logging.info(' Final (TOT. EPOCHS {})::  Loss: {:.8f} \\t   AUC (Test) :{:.4f} '\n",
    "            .format(EPOCH, loss_epoch / n_batches, Tst_auc_score))\n",
    "\n",
    "    print(' Final (TOT. EPOCHS {})::  Loss: {:.8f} \\t   AUC (Test) :{:.4f} '\n",
    "            .format(EPOCH, loss_epoch / n_batches, Tst_auc_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Training data: class 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************\n",
      "Normal class: \n",
      "5\n",
      "lam = 0.05,lr = 0.001, Cu = 1.0\n",
      "Exp No: 0\n",
      "Files already downloaded and verified\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Files already downloaded and verified\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  Epoch 0/350\t Time: 1.735\t Loss: 7.65478498 \t Test AUC :0.5589 \n",
      "  Epoch 50/350\t Time: 1.705\t Loss: 5.76606043 \t Test AUC :0.6440 \n",
      "  Epoch 100/350\t Time: 1.705\t Loss: 4.76864526 \t Test AUC :0.6528 \n",
      "  Epoch 150/350\t Time: 1.752\t Loss: 3.94667141 \t Test AUC :0.6525 \n",
      "  Epoch 200/350\t Time: 1.664\t Loss: 3.26845138 \t Test AUC :0.6537 \n",
      "  Epoch 250/350\t Time: 1.608\t Loss: 2.70823157 \t Test AUC :0.6492 \n",
      "  Epoch 300/350\t Time: 1.671\t Loss: 2.24520887 \t Test AUC :0.6396 \n",
      "  Epoch 350/350\t Time: 1.843\t Loss: 1.86254630 \t Test AUC :0.6209 \n",
      " Final (TOT. EPOCHS 350)::  Loss: 1.86254630 \t   AUC (Test) :0.6209 \n"
     ]
    }
   ],
   "source": [
    "EPOCH = 350 \n",
    "BATCH_SIZEU = 256\n",
    "BATCH_SIZE = 256\n",
    "normalclass = 5\n",
    "\n",
    "print('********************************')\n",
    "print('Normal class: ')\n",
    "print(normalclass)\n",
    "logging.info('********************************')\n",
    "logging.info('Normal class: ')\n",
    "logging.info(normalclass)\n",
    "\n",
    "print('lam = {},lr = {}, Cu = {}'.format(0.05,0.001,1.0))\n",
    "logging.info('lam = {},lr = {}, Cu = {}'.format(0.05,0.001,1.0))\n",
    "\n",
    "tst_score = []\n",
    "\n",
    "transform = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "    \n",
    "for exp in range(n_reps):\n",
    "    \n",
    "    print('Exp No: {}'.format(exp))\n",
    "    logging.info('Exp No: {}'.format(exp))\n",
    "\n",
    "    ###############################################################################################\n",
    "    #                                     Train Data\n",
    "    ###############################################################################################\n",
    "    cifar_train =  torchvision.datasets.CIFAR10(root=\"./cifar/cifar_train\", train=True, download=True, \n",
    "                                              transform=transform)\n",
    "\n",
    "    balanced_batch_sampler = BalancedBatchSampler(cifar_train, 10, n_samples = BATCH_SIZE, class_id = [normalclass])\n",
    "    train_loader = torch.utils.data.DataLoader(cifar_train, batch_sampler=balanced_batch_sampler, pin_memory=True, num_workers=NUM_WORKERS)\n",
    "\n",
    "    \n",
    "    ###############################################################################################\n",
    "    #                                     Evaluations on :-\n",
    "    ###############################################################################################\n",
    "    # Test Data\n",
    "    test_dat = torchvision.datasets.CIFAR10('./cifar', train=False, download=True, transform=transform)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_dat, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True, num_workers=NUM_WORKERS)\n",
    "    \n",
    "    \n",
    "    # Train (used to Evaluate i.e. TP_rate etc.)\n",
    "    balanced_batch_sampler_eval = BalancedBatchSampler(cifar_train, 10, n_samples = BATCH_SIZE, class_id = [normalclass],allSamples=True)\n",
    "    train_loader_eval = torch.utils.data.DataLoader(cifar_train, batch_sampler=balanced_batch_sampler_eval, pin_memory=True, num_workers=NUM_WORKERS)\n",
    "\n",
    "    \n",
    "    lam = 0.05\n",
    "    lr = 0.001\n",
    "    Cu = 1.0\n",
    "    \n",
    "    \n",
    "    eps = 0.0\n",
    "    dpos = 1.0-eps\n",
    "    dneg = -(1.0+eps)\n",
    "\n",
    "    net = CIFAR10_LeNet()\n",
    "    net = net.to(device)\n",
    "\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr = lr)  \n",
    "\n",
    "    train_loss = HingeLoss()\n",
    "    unlabeled_posloss = HingeLoss()\n",
    "    unlabeled_negloss = HingeLoss()\n",
    "\n",
    "    for epoch in range(EPOCH+1):\n",
    "\n",
    "        loss_epoch = 0.0\n",
    "        n_batches = 0\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        for (i,data) in enumerate(train_loader):\n",
    "\n",
    "            # Training Data\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            ytr = np.ones(len(inputs))\n",
    "            ytr = torch.from_numpy(ytr).to(device).float()\n",
    "            outputs = net(inputs) \n",
    "\n",
    "            XU = torch.from_numpy(np.random.randn(BATCH_SIZEU, 3, 32, 32))\n",
    "\n",
    "            XU = XU.to(device).float()\n",
    "            outputsU = net(XU)\n",
    "            yunppos = np.ones(BATCH_SIZE)\n",
    "            yunpneg = -np.ones(BATCH_SIZE)\n",
    "            yupos = torch.from_numpy(yunppos).to(device).float()\n",
    "            yuneg = torch.from_numpy(yunpneg).to(device).float()\n",
    "\n",
    "            \n",
    "            # Zero the network parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Loss\n",
    "            loss = train_loss(outputs,ytr,smooth = False)\n",
    "\n",
    "            # Unlabeled data\n",
    "            lossunlab = unlabeled_posloss(outputsU,yupos,dpos,smooth = False) + unlabeled_negloss(outputsU,yuneg,dneg,smooth = False)\n",
    "            loss+=Cu*lossunlab\n",
    "\n",
    "            lam = torch.tensor(lam).to(device)\n",
    "            l2_reg = torch.tensor(0.).to(device)\n",
    "\n",
    "            for name, param in net.named_parameters():\n",
    "                l2_reg += torch.norm(param)**2\n",
    "\n",
    "            loss += lam * l2_reg\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_epoch += loss.item()\n",
    "            n_batches += 1\n",
    "\n",
    "\n",
    "        if epoch%50==0:\n",
    "\n",
    "            idx_label_score = []\n",
    "            net.eval()\n",
    "            with torch.no_grad():\n",
    "                for data in test_loader:\n",
    "                    inputs, labels = data\n",
    "\n",
    "                    labels = labels.cpu().data.numpy()\n",
    "                    labels = (labels==normalclass).astype(int)\n",
    "\n",
    "                    inputs = inputs.to(device)\n",
    "                    outputs = net(inputs)\n",
    "                    scores = outputs.data.cpu().numpy()\n",
    "                    idx_label_score += list(zip(labels.tolist(),\n",
    "                                                scores.tolist()))\n",
    "\n",
    "            tstlabels, scores = zip(*idx_label_score)\n",
    "            tstlabels = np.array(tstlabels)\n",
    "\n",
    "            tstlabels[np.where(tstlabels==0)]=-1.0\n",
    "            scores = np.array(scores)\n",
    "\n",
    "            Tst_auc_score = evalmetrics(tstlabels,scores.flatten())\n",
    "\n",
    "            # log epoch statistics\n",
    "            epoch_train_time = time.time() - epoch_start_time\n",
    "            logging.info('  Epoch {}/{}\\t Time: {:.3f}\\t Loss: {:.8f} \\t Test AUC :{:.4f} '\n",
    "                        .format(epoch , EPOCH, epoch_train_time, loss_epoch / n_batches, Tst_auc_score))\n",
    "\n",
    "            print('  Epoch {}/{}\\t Time: {:.3f}\\t Loss: {:.8f} \\t Test AUC :{:.4f} '\n",
    "                        .format(epoch , EPOCH, epoch_train_time, loss_epoch / n_batches, Tst_auc_score))\n",
    "\n",
    "\n",
    "    # Final Solution\n",
    "    logging.info(' Final (TOT. EPOCHS {})::  Loss: {:.8f} \\t   AUC (Test) :{:.4f} '\n",
    "            .format(EPOCH, loss_epoch / n_batches, Tst_auc_score))\n",
    "\n",
    "    print(' Final (TOT. EPOCHS {})::  Loss: {:.8f} \\t   AUC (Test) :{:.4f} '\n",
    "            .format(EPOCH, loss_epoch / n_batches, Tst_auc_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Training data: class 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************\n",
      "Normal class: \n",
      "6\n",
      "lam = 0.1,lr = 0.005, Cu = 0.5\n",
      "Exp No: 0\n",
      "Files already downloaded and verified\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Files already downloaded and verified\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  Epoch 0/350\t Time: 1.847\t Loss: 14.01391662 \t Test AUC :0.4667 \n",
      "  Epoch 50/350\t Time: 1.695\t Loss: 2.09322165 \t Test AUC :0.7159 \n",
      "  Epoch 100/350\t Time: 1.685\t Loss: 0.36160782 \t Test AUC :0.7269 \n",
      "  Epoch 150/350\t Time: 1.836\t Loss: 0.10982294 \t Test AUC :0.7475 \n",
      "  Epoch 200/350\t Time: 1.684\t Loss: 0.07933822 \t Test AUC :0.7557 \n",
      "  Epoch 250/350\t Time: 1.690\t Loss: 0.06756092 \t Test AUC :0.7619 \n",
      "  Epoch 300/350\t Time: 1.698\t Loss: 0.06784672 \t Test AUC :0.7605 \n",
      "  Epoch 350/350\t Time: 1.763\t Loss: 0.06705865 \t Test AUC :0.7641 \n",
      " Final (TOT. EPOCHS 350)::  Loss: 0.06705865 \t   AUC (Test) :0.7641 \n"
     ]
    }
   ],
   "source": [
    "EPOCH = 350 \n",
    "BATCH_SIZEU = 256\n",
    "BATCH_SIZE = 256\n",
    "normalclass = 6\n",
    "\n",
    "print('********************************')\n",
    "print('Normal class: ')\n",
    "print(normalclass)\n",
    "logging.info('********************************')\n",
    "logging.info('Normal class: ')\n",
    "logging.info(normalclass)\n",
    "\n",
    "print('lam = {},lr = {}, Cu = {}'.format(0.1,0.005,0.5))\n",
    "logging.info('lam = {},lr = {}, Cu = {}'.format(0.1,0.005,0.5))\n",
    "\n",
    "\n",
    "transform = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "    \n",
    "for exp in range(n_reps):\n",
    "    \n",
    "    print('Exp No: {}'.format(exp))\n",
    "    logging.info('Exp No: {}'.format(exp))\n",
    "\n",
    "    ###############################################################################################\n",
    "    #                                     Train Data\n",
    "    ###############################################################################################\n",
    "    cifar_train =  torchvision.datasets.CIFAR10(root=\"./cifar/cifar_train\", train=True, download=True, \n",
    "                                              transform=transform)\n",
    "\n",
    "    balanced_batch_sampler = BalancedBatchSampler(cifar_train, 10, n_samples = BATCH_SIZE, class_id = [normalclass])\n",
    "    train_loader = torch.utils.data.DataLoader(cifar_train, batch_sampler=balanced_batch_sampler, pin_memory=True, num_workers=NUM_WORKERS)\n",
    "\n",
    "    \n",
    "    ###############################################################################################\n",
    "    #                                     Evaluations on :-\n",
    "    ###############################################################################################\n",
    "    # Test Data\n",
    "    test_dat = torchvision.datasets.CIFAR10('./cifar', train=False, download=True, transform=transform)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_dat, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True, num_workers=NUM_WORKERS)\n",
    "    \n",
    "    \n",
    "    # Train (used to Evaluate i.e. TP_rate etc.)\n",
    "    balanced_batch_sampler_eval = BalancedBatchSampler(cifar_train, 10, n_samples = BATCH_SIZE, class_id = [normalclass],allSamples=True)\n",
    "    train_loader_eval = torch.utils.data.DataLoader(cifar_train, batch_sampler=balanced_batch_sampler_eval, pin_memory=True, num_workers=NUM_WORKERS)\n",
    "\n",
    "    \n",
    "    lam = 0.1\n",
    "    lr = 0.005\n",
    "    Cu = 0.5\n",
    "    \n",
    "    \n",
    "    eps = 0.0\n",
    "    dpos = 1.0-eps\n",
    "    dneg = -(1.0+eps)\n",
    "\n",
    "    net = CIFAR10_LeNet()\n",
    "    net = net.to(device)\n",
    "\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr = lr)  \n",
    "\n",
    "    train_loss = HingeLoss()\n",
    "    unlabeled_posloss = HingeLoss()\n",
    "    unlabeled_negloss = HingeLoss()\n",
    "\n",
    "    for epoch in range(EPOCH+1):\n",
    "\n",
    "        loss_epoch = 0.0\n",
    "        n_batches = 0\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        for (i,data) in enumerate(train_loader):\n",
    "\n",
    "            # Training Data\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            ytr = np.ones(len(inputs))\n",
    "            ytr = torch.from_numpy(ytr).to(device).float()\n",
    "            outputs = net(inputs) \n",
    "\n",
    "            XU = torch.from_numpy(np.random.randn(BATCH_SIZEU, 3, 32, 32))\n",
    "\n",
    "            XU = XU.to(device).float()\n",
    "            outputsU = net(XU)\n",
    "            yunppos = np.ones(BATCH_SIZE)\n",
    "            yunpneg = -np.ones(BATCH_SIZE)\n",
    "            yupos = torch.from_numpy(yunppos).to(device).float()\n",
    "            yuneg = torch.from_numpy(yunpneg).to(device).float()\n",
    "\n",
    "            \n",
    "            # Zero the network parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Loss\n",
    "            loss = train_loss(outputs,ytr,smooth = False)\n",
    "\n",
    "            # Unlabeled data\n",
    "            lossunlab = unlabeled_posloss(outputsU,yupos,dpos,smooth = False) + unlabeled_negloss(outputsU,yuneg,dneg,smooth = False)\n",
    "            loss+=Cu*lossunlab\n",
    "\n",
    "            lam = torch.tensor(lam).to(device)\n",
    "            l2_reg = torch.tensor(0.).to(device)\n",
    "\n",
    "            for name, param in net.named_parameters():\n",
    "                l2_reg += torch.norm(param)**2\n",
    "\n",
    "            loss += lam * l2_reg\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_epoch += loss.item()\n",
    "            n_batches += 1\n",
    "\n",
    "\n",
    "        if epoch%50==0:\n",
    "\n",
    "            idx_label_score = []\n",
    "            net.eval()\n",
    "            with torch.no_grad():\n",
    "                for data in test_loader:\n",
    "                    inputs, labels = data\n",
    "\n",
    "                    labels = labels.cpu().data.numpy()\n",
    "                    labels = (labels==normalclass).astype(int)\n",
    "\n",
    "                    inputs = inputs.to(device)\n",
    "                    outputs = net(inputs)\n",
    "                    scores = outputs.data.cpu().numpy()\n",
    "                    idx_label_score += list(zip(labels.tolist(),\n",
    "                                                scores.tolist()))\n",
    "\n",
    "            tstlabels, scores = zip(*idx_label_score)\n",
    "            tstlabels = np.array(tstlabels)\n",
    "\n",
    "            tstlabels[np.where(tstlabels==0)]=-1.0\n",
    "            scores = np.array(scores)\n",
    "\n",
    "            Tst_auc_score = evalmetrics(tstlabels,scores.flatten())\n",
    "\n",
    "            # log epoch statistics\n",
    "            epoch_train_time = time.time() - epoch_start_time\n",
    "            logging.info('  Epoch {}/{}\\t Time: {:.3f}\\t Loss: {:.8f} \\t Test AUC :{:.4f} '\n",
    "                        .format(epoch , EPOCH, epoch_train_time, loss_epoch / n_batches, Tst_auc_score))\n",
    "\n",
    "            print('  Epoch {}/{}\\t Time: {:.3f}\\t Loss: {:.8f} \\t Test AUC :{:.4f} '\n",
    "                        .format(epoch , EPOCH, epoch_train_time, loss_epoch / n_batches, Tst_auc_score))\n",
    "\n",
    "\n",
    "    # Final Solution\n",
    "    logging.info(' Final (TOT. EPOCHS {})::  Loss: {:.8f} \\t   AUC (Test) :{:.4f} '\n",
    "            .format(EPOCH, loss_epoch / n_batches, Tst_auc_score))\n",
    "\n",
    "    print(' Final (TOT. EPOCHS {})::  Loss: {:.8f} \\t   AUC (Test) :{:.4f} '\n",
    "            .format(EPOCH, loss_epoch / n_batches, Tst_auc_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 Training data: class 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************\n",
      "Normal class: \n",
      "7\n",
      "lam = 0.05,lr = 0.0005, Cu = 1.0\n",
      "Exp No: 0\n",
      "Files already downloaded and verified\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Files already downloaded and verified\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  Epoch 0/350\t Time: 1.730\t Loss: 8.35768243 \t Test AUC :0.6058 \n",
      "  Epoch 50/350\t Time: 1.688\t Loss: 6.36298385 \t Test AUC :0.6633 \n",
      "  Epoch 100/350\t Time: 1.664\t Loss: 5.78328231 \t Test AUC :0.6555 \n",
      "  Epoch 150/350\t Time: 1.847\t Loss: 5.26072843 \t Test AUC :0.6501 \n",
      "  Epoch 200/350\t Time: 1.654\t Loss: 4.78545984 \t Test AUC :0.6482 \n",
      "  Epoch 250/350\t Time: 1.688\t Loss: 4.35467921 \t Test AUC :0.6496 \n",
      "  Epoch 300/350\t Time: 1.684\t Loss: 3.96300528 \t Test AUC :0.6515 \n",
      "  Epoch 350/350\t Time: 1.779\t Loss: 3.60627202 \t Test AUC :0.6533 \n",
      " Final (TOT. EPOCHS 350)::  Loss: 3.60627202 \t   AUC (Test) :0.6533 \n"
     ]
    }
   ],
   "source": [
    "EPOCH = 350 \n",
    "BATCH_SIZEU = 256\n",
    "BATCH_SIZE = 256\n",
    "normalclass = 7\n",
    "\n",
    "print('********************************')\n",
    "print('Normal class: ')\n",
    "print(normalclass)\n",
    "logging.info('********************************')\n",
    "logging.info('Normal class: ')\n",
    "logging.info(normalclass)\n",
    "\n",
    "print('lam = {},lr = {}, Cu = {}'.format(0.05,0.0005,1.0))\n",
    "logging.info('lam = {},lr = {}, Cu = {}'.format(0.05,0.0005,1.0))\n",
    "\n",
    "\n",
    "transform = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "    \n",
    "for exp in range(n_reps):\n",
    "    \n",
    "    print('Exp No: {}'.format(exp))\n",
    "    logging.info('Exp No: {}'.format(exp))\n",
    "\n",
    "    ###############################################################################################\n",
    "    #                                     Train Data\n",
    "    ###############################################################################################\n",
    "    cifar_train =  torchvision.datasets.CIFAR10(root=\"./cifar/cifar_train\", train=True, download=True, \n",
    "                                              transform=transform)\n",
    "\n",
    "    balanced_batch_sampler = BalancedBatchSampler(cifar_train, 10, n_samples = BATCH_SIZE, class_id = [normalclass])\n",
    "    train_loader = torch.utils.data.DataLoader(cifar_train, batch_sampler=balanced_batch_sampler, pin_memory=True, num_workers=NUM_WORKERS)\n",
    "\n",
    "    \n",
    "    ###############################################################################################\n",
    "    #                                     Evaluations on :-\n",
    "    ###############################################################################################\n",
    "    # Test Data\n",
    "    test_dat = torchvision.datasets.CIFAR10('./cifar', train=False, download=True, transform=transform)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_dat, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True, num_workers=NUM_WORKERS)\n",
    "    \n",
    "    \n",
    "    # Train (used to Evaluate i.e. TP_rate etc.)\n",
    "    balanced_batch_sampler_eval = BalancedBatchSampler(cifar_train, 10, n_samples = BATCH_SIZE, class_id = [normalclass],allSamples=True)\n",
    "    train_loader_eval = torch.utils.data.DataLoader(cifar_train, batch_sampler=balanced_batch_sampler_eval, pin_memory=True, num_workers=NUM_WORKERS)\n",
    "\n",
    "    \n",
    "    lam = 0.05\n",
    "    lr = 0.0005\n",
    "    Cu = 1.0\n",
    "    \n",
    "    \n",
    "    eps = 0.0\n",
    "    dpos = 1.0-eps\n",
    "    dneg = -(1.0+eps)\n",
    "\n",
    "    net = CIFAR10_LeNet()\n",
    "    net = net.to(device)\n",
    "\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr = lr)  \n",
    "\n",
    "    train_loss = HingeLoss()\n",
    "    unlabeled_posloss = HingeLoss()\n",
    "    unlabeled_negloss = HingeLoss()\n",
    "\n",
    "    for epoch in range(EPOCH+1):\n",
    "\n",
    "        loss_epoch = 0.0\n",
    "        n_batches = 0\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        for (i,data) in enumerate(train_loader):\n",
    "\n",
    "            # Training Data\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            ytr = np.ones(len(inputs))\n",
    "            ytr = torch.from_numpy(ytr).to(device).float()\n",
    "            outputs = net(inputs) \n",
    "\n",
    "            XU = torch.from_numpy(np.random.randn(BATCH_SIZEU, 3, 32, 32))\n",
    "\n",
    "            XU = XU.to(device).float()\n",
    "            outputsU = net(XU)\n",
    "            yunppos = np.ones(BATCH_SIZE)\n",
    "            yunpneg = -np.ones(BATCH_SIZE)\n",
    "            yupos = torch.from_numpy(yunppos).to(device).float()\n",
    "            yuneg = torch.from_numpy(yunpneg).to(device).float()\n",
    "\n",
    "            \n",
    "            # Zero the network parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Loss\n",
    "            loss = train_loss(outputs,ytr,smooth = False)\n",
    "\n",
    "            # Unlabeled data\n",
    "            lossunlab = unlabeled_posloss(outputsU,yupos,dpos,smooth = False) + unlabeled_negloss(outputsU,yuneg,dneg,smooth = False)\n",
    "            loss+=Cu*lossunlab\n",
    "\n",
    "            lam = torch.tensor(lam).to(device)\n",
    "            l2_reg = torch.tensor(0.).to(device)\n",
    "\n",
    "            for name, param in net.named_parameters():\n",
    "                l2_reg += torch.norm(param)**2\n",
    "\n",
    "            loss += lam * l2_reg\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_epoch += loss.item()\n",
    "            n_batches += 1\n",
    "\n",
    "\n",
    "        if epoch%50==0:\n",
    "\n",
    "            idx_label_score = []\n",
    "            net.eval()\n",
    "            with torch.no_grad():\n",
    "                for data in test_loader:\n",
    "                    inputs, labels = data\n",
    "\n",
    "                    labels = labels.cpu().data.numpy()\n",
    "                    labels = (labels==normalclass).astype(int)\n",
    "\n",
    "                    inputs = inputs.to(device)\n",
    "                    outputs = net(inputs)\n",
    "                    scores = outputs.data.cpu().numpy()\n",
    "                    idx_label_score += list(zip(labels.tolist(),\n",
    "                                                scores.tolist()))\n",
    "\n",
    "            tstlabels, scores = zip(*idx_label_score)\n",
    "            tstlabels = np.array(tstlabels)\n",
    "\n",
    "            tstlabels[np.where(tstlabels==0)]=-1.0\n",
    "            scores = np.array(scores)\n",
    "\n",
    "            Tst_auc_score = evalmetrics(tstlabels,scores.flatten())\n",
    "\n",
    "            # log epoch statistics\n",
    "            epoch_train_time = time.time() - epoch_start_time\n",
    "            logging.info('  Epoch {}/{}\\t Time: {:.3f}\\t Loss: {:.8f} \\t Test AUC :{:.4f} '\n",
    "                        .format(epoch , EPOCH, epoch_train_time, loss_epoch / n_batches, Tst_auc_score))\n",
    "\n",
    "            print('  Epoch {}/{}\\t Time: {:.3f}\\t Loss: {:.8f} \\t Test AUC :{:.4f} '\n",
    "                        .format(epoch , EPOCH, epoch_train_time, loss_epoch / n_batches, Tst_auc_score))\n",
    "\n",
    "\n",
    "    # Final Solution\n",
    "    logging.info(' Final (TOT. EPOCHS {})::  Loss: {:.8f} \\t   AUC (Test) :{:.4f} '\n",
    "            .format(EPOCH, loss_epoch / n_batches, Tst_auc_score))\n",
    "\n",
    "    print(' Final (TOT. EPOCHS {})::  Loss: {:.8f} \\t   AUC (Test) :{:.4f} '\n",
    "            .format(EPOCH, loss_epoch / n_batches, Tst_auc_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8 Training data: class 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************\n",
      "Normal class: \n",
      "8\n",
      "lam = 0.05,lr = 0.005, Cu = 0.1\n",
      "Exp No: 0\n",
      "Files already downloaded and verified\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Files already downloaded and verified\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  Epoch 0/350\t Time: 1.784\t Loss: 7.03883036 \t Test AUC :0.6119 \n",
      "  Epoch 50/350\t Time: 1.674\t Loss: 2.67228883 \t Test AUC :0.5226 \n",
      "  Epoch 100/350\t Time: 1.654\t Loss: 1.04039698 \t Test AUC :0.5581 \n",
      "  Epoch 150/350\t Time: 1.750\t Loss: 0.40995010 \t Test AUC :0.6883 \n",
      "  Epoch 200/350\t Time: 1.713\t Loss: 0.16630792 \t Test AUC :0.7148 \n",
      "  Epoch 250/350\t Time: 1.641\t Loss: 0.07294629 \t Test AUC :0.7331 \n",
      "  Epoch 300/350\t Time: 1.798\t Loss: 0.03935403 \t Test AUC :0.7729 \n",
      "  Epoch 350/350\t Time: 1.631\t Loss: 0.02281585 \t Test AUC :0.8104 \n",
      " Final (TOT. EPOCHS 350)::  Loss: 0.02281585 \t   AUC (Test) :0.8104 \n"
     ]
    }
   ],
   "source": [
    "EPOCH = 350 \n",
    "BATCH_SIZEU = 256\n",
    "BATCH_SIZE = 256\n",
    "normalclass = 8\n",
    "\n",
    "print('********************************')\n",
    "print('Normal class: ')\n",
    "print(normalclass)\n",
    "logging.info('********************************')\n",
    "logging.info('Normal class: ')\n",
    "logging.info(normalclass)\n",
    "\n",
    "print('lam = {},lr = {}, Cu = {}'.format(0.05,0.005,0.1))\n",
    "logging.info('lam = {},lr = {}, Cu = {}'.format(0.05,0.005,0.1))\n",
    "\n",
    "\n",
    "transform = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "    \n",
    "for exp in range(n_reps):\n",
    "    \n",
    "    print('Exp No: {}'.format(exp))\n",
    "    logging.info('Exp No: {}'.format(exp))\n",
    "\n",
    "    ###############################################################################################\n",
    "    #                                     Train Data\n",
    "    ###############################################################################################\n",
    "    cifar_train =  torchvision.datasets.CIFAR10(root=\"./cifar/cifar_train\", train=True, download=True, \n",
    "                                              transform=transform)\n",
    "\n",
    "    balanced_batch_sampler = BalancedBatchSampler(cifar_train, 10, n_samples = BATCH_SIZE, class_id = [normalclass])\n",
    "    train_loader = torch.utils.data.DataLoader(cifar_train, batch_sampler=balanced_batch_sampler, pin_memory=True, num_workers=NUM_WORKERS)\n",
    "\n",
    "    \n",
    "    ###############################################################################################\n",
    "    #                                     Evaluations on :-\n",
    "    ###############################################################################################\n",
    "    # Test Data\n",
    "    test_dat = torchvision.datasets.CIFAR10('./cifar', train=False, download=True, transform=transform)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_dat, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True, num_workers=NUM_WORKERS)\n",
    "    \n",
    "    \n",
    "    # Train (used to Evaluate i.e. TP_rate etc.)\n",
    "    balanced_batch_sampler_eval = BalancedBatchSampler(cifar_train, 10, n_samples = BATCH_SIZE, class_id = [normalclass],allSamples=True)\n",
    "    train_loader_eval = torch.utils.data.DataLoader(cifar_train, batch_sampler=balanced_batch_sampler_eval, pin_memory=True, num_workers=NUM_WORKERS)\n",
    "\n",
    "    \n",
    "    lam = 0.05\n",
    "    lr = 0.005\n",
    "    Cu = 0.1\n",
    "    \n",
    "    \n",
    "    eps = 0.0\n",
    "    dpos = 1.0-eps\n",
    "    dneg = -(1.0+eps)\n",
    "\n",
    "    net = CIFAR10_LeNet()\n",
    "    net = net.to(device)\n",
    "\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr = lr)  \n",
    "\n",
    "    train_loss = HingeLoss()\n",
    "    unlabeled_posloss = HingeLoss()\n",
    "    unlabeled_negloss = HingeLoss()\n",
    "\n",
    "    for epoch in range(EPOCH+1):\n",
    "\n",
    "        loss_epoch = 0.0\n",
    "        n_batches = 0\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        for (i,data) in enumerate(train_loader):\n",
    "\n",
    "            # Training Data\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            ytr = np.ones(len(inputs))\n",
    "            ytr = torch.from_numpy(ytr).to(device).float()\n",
    "            outputs = net(inputs) \n",
    "\n",
    "            XU = torch.from_numpy(np.random.randn(BATCH_SIZEU, 3, 32, 32))\n",
    "\n",
    "            XU = XU.to(device).float()\n",
    "            outputsU = net(XU)\n",
    "            yunppos = np.ones(BATCH_SIZE)\n",
    "            yunpneg = -np.ones(BATCH_SIZE)\n",
    "            yupos = torch.from_numpy(yunppos).to(device).float()\n",
    "            yuneg = torch.from_numpy(yunpneg).to(device).float()\n",
    "\n",
    "            \n",
    "            # Zero the network parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Loss\n",
    "            loss = train_loss(outputs,ytr,smooth = False)\n",
    "\n",
    "            # Unlabeled data\n",
    "            lossunlab = unlabeled_posloss(outputsU,yupos,dpos,smooth = False) + unlabeled_negloss(outputsU,yuneg,dneg,smooth = False)\n",
    "            loss+=Cu*lossunlab\n",
    "\n",
    "            lam = torch.tensor(lam).to(device)\n",
    "            l2_reg = torch.tensor(0.).to(device)\n",
    "\n",
    "            for name, param in net.named_parameters():\n",
    "                l2_reg += torch.norm(param)**2\n",
    "\n",
    "            loss += lam * l2_reg\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_epoch += loss.item()\n",
    "            n_batches += 1\n",
    "\n",
    "\n",
    "        if epoch%50==0:\n",
    "\n",
    "            idx_label_score = []\n",
    "            net.eval()\n",
    "            with torch.no_grad():\n",
    "                for data in test_loader:\n",
    "                    inputs, labels = data\n",
    "\n",
    "                    labels = labels.cpu().data.numpy()\n",
    "                    labels = (labels==normalclass).astype(int)\n",
    "\n",
    "                    inputs = inputs.to(device)\n",
    "                    outputs = net(inputs)\n",
    "                    scores = outputs.data.cpu().numpy()\n",
    "                    idx_label_score += list(zip(labels.tolist(),\n",
    "                                                scores.tolist()))\n",
    "\n",
    "            tstlabels, scores = zip(*idx_label_score)\n",
    "            tstlabels = np.array(tstlabels)\n",
    "\n",
    "            tstlabels[np.where(tstlabels==0)]=-1.0\n",
    "            scores = np.array(scores)\n",
    "\n",
    "            Tst_auc_score = evalmetrics(tstlabels,scores.flatten())\n",
    "\n",
    "            # log epoch statistics\n",
    "            epoch_train_time = time.time() - epoch_start_time\n",
    "            logging.info('  Epoch {}/{}\\t Time: {:.3f}\\t Loss: {:.8f} \\t Test AUC :{:.4f} '\n",
    "                        .format(epoch , EPOCH, epoch_train_time, loss_epoch / n_batches, Tst_auc_score))\n",
    "\n",
    "            print('  Epoch {}/{}\\t Time: {:.3f}\\t Loss: {:.8f} \\t Test AUC :{:.4f} '\n",
    "                        .format(epoch , EPOCH, epoch_train_time, loss_epoch / n_batches, Tst_auc_score))\n",
    "\n",
    "\n",
    "    # Final Solution\n",
    "    logging.info(' Final (TOT. EPOCHS {})::  Loss: {:.8f} \\t   AUC (Test) :{:.4f} '\n",
    "            .format(EPOCH, loss_epoch / n_batches, Tst_auc_score))\n",
    "\n",
    "    print(' Final (TOT. EPOCHS {})::  Loss: {:.8f} \\t   AUC (Test) :{:.4f} '\n",
    "            .format(EPOCH, loss_epoch / n_batches, Tst_auc_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.9 Training data: class 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************\n",
      "Normal class: \n",
      "9\n",
      "lam = 0.05,lr = 0.0001, Cu = 0.05\n",
      "Exp No: 0\n",
      "Files already downloaded and verified\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Files already downloaded and verified\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "  Epoch 0/350\t Time: 1.705\t Loss: 7.77868504 \t Test AUC :0.5627 \n",
      "  Epoch 50/350\t Time: 1.691\t Loss: 6.80146448 \t Test AUC :0.7978 \n",
      "  Epoch 100/350\t Time: 1.811\t Loss: 6.67330820 \t Test AUC :0.7994 \n",
      "  Epoch 150/350\t Time: 1.686\t Loss: 6.54768683 \t Test AUC :0.8003 \n",
      "  Epoch 200/350\t Time: 1.730\t Loss: 6.42442927 \t Test AUC :0.8010 \n",
      "  Epoch 250/350\t Time: 1.713\t Loss: 6.30347317 \t Test AUC :0.8015 \n",
      "  Epoch 300/350\t Time: 1.847\t Loss: 6.18480507 \t Test AUC :0.8016 \n",
      "  Epoch 350/350\t Time: 1.662\t Loss: 6.06841050 \t Test AUC :0.8016 \n",
      " Final (TOT. EPOCHS 350)::  Loss: 6.06841050 \t   AUC (Test) :0.8016 \n"
     ]
    }
   ],
   "source": [
    "EPOCH = 350 \n",
    "BATCH_SIZEU = 256\n",
    "BATCH_SIZE = 256\n",
    "normalclass = 9\n",
    "\n",
    "print('********************************')\n",
    "print('Normal class: ')\n",
    "print(normalclass)\n",
    "logging.info('********************************')\n",
    "logging.info('Normal class: ')\n",
    "logging.info(normalclass)\n",
    "\n",
    "print('lam = {},lr = {}, Cu = {}'.format(0.05,0.0001,0.05))\n",
    "logging.info('lam = {},lr = {}, Cu = {}'.format(0.05,0.0001,0.05))\n",
    "\n",
    "\n",
    "transform = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "    \n",
    "for exp in range(n_reps):\n",
    "    \n",
    "    print('Exp No: {}'.format(exp))\n",
    "    logging.info('Exp No: {}'.format(exp))\n",
    "\n",
    "    ###############################################################################################\n",
    "    #                                     Train Data\n",
    "    ###############################################################################################\n",
    "    cifar_train =  torchvision.datasets.CIFAR10(root=\"./cifar/cifar_train\", train=True, download=True, \n",
    "                                              transform=transform)\n",
    "\n",
    "    balanced_batch_sampler = BalancedBatchSampler(cifar_train, 10, n_samples = BATCH_SIZE, class_id = [normalclass])\n",
    "    train_loader = torch.utils.data.DataLoader(cifar_train, batch_sampler=balanced_batch_sampler, pin_memory=True, num_workers=NUM_WORKERS)\n",
    "\n",
    "    \n",
    "    ###############################################################################################\n",
    "    #                                     Evaluations on :-\n",
    "    ###############################################################################################\n",
    "    # Test Data\n",
    "    test_dat = torchvision.datasets.CIFAR10('./cifar', train=False, download=True, transform=transform)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_dat, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True, num_workers=NUM_WORKERS)\n",
    "    \n",
    "    \n",
    "    # Train (used to Evaluate i.e. TP_rate etc.)\n",
    "    balanced_batch_sampler_eval = BalancedBatchSampler(cifar_train, 10, n_samples = BATCH_SIZE, class_id = [normalclass],allSamples=True)\n",
    "    train_loader_eval = torch.utils.data.DataLoader(cifar_train, batch_sampler=balanced_batch_sampler_eval, pin_memory=True, num_workers=NUM_WORKERS)\n",
    "\n",
    "    \n",
    "    lam = 0.05\n",
    "    lr = 0.0001\n",
    "    Cu = 0.05\n",
    "    \n",
    "    \n",
    "    eps = 0.0\n",
    "    dpos = 1.0-eps\n",
    "    dneg = -(1.0+eps)\n",
    "\n",
    "    net = CIFAR10_LeNet()\n",
    "    net = net.to(device)\n",
    "\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr = lr)  \n",
    "\n",
    "    train_loss = HingeLoss()\n",
    "    unlabeled_posloss = HingeLoss()\n",
    "    unlabeled_negloss = HingeLoss()\n",
    "\n",
    "    for epoch in range(EPOCH+1):\n",
    "\n",
    "        loss_epoch = 0.0\n",
    "        n_batches = 0\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        for (i,data) in enumerate(train_loader):\n",
    "\n",
    "            # Training Data\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            ytr = np.ones(len(inputs))\n",
    "            ytr = torch.from_numpy(ytr).to(device).float()\n",
    "            outputs = net(inputs) \n",
    "\n",
    "            XU = torch.from_numpy(np.random.randn(BATCH_SIZEU, 3, 32, 32))\n",
    "\n",
    "            XU = XU.to(device).float()\n",
    "            outputsU = net(XU)\n",
    "            yunppos = np.ones(BATCH_SIZE)\n",
    "            yunpneg = -np.ones(BATCH_SIZE)\n",
    "            yupos = torch.from_numpy(yunppos).to(device).float()\n",
    "            yuneg = torch.from_numpy(yunpneg).to(device).float()\n",
    "\n",
    "            \n",
    "            # Zero the network parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Loss\n",
    "            loss = train_loss(outputs,ytr,smooth = False)\n",
    "\n",
    "            # Unlabeled data\n",
    "            lossunlab = unlabeled_posloss(outputsU,yupos,dpos,smooth = False) + unlabeled_negloss(outputsU,yuneg,dneg,smooth = False)\n",
    "            loss+=Cu*lossunlab\n",
    "\n",
    "            lam = torch.tensor(lam).to(device)\n",
    "            l2_reg = torch.tensor(0.).to(device)\n",
    "\n",
    "            for name, param in net.named_parameters():\n",
    "                l2_reg += torch.norm(param)**2\n",
    "\n",
    "            loss += lam * l2_reg\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_epoch += loss.item()\n",
    "            n_batches += 1\n",
    "\n",
    "\n",
    "        if epoch%50==0:\n",
    "\n",
    "            idx_label_score = []\n",
    "            net.eval()\n",
    "            with torch.no_grad():\n",
    "                for data in test_loader:\n",
    "                    inputs, labels = data\n",
    "\n",
    "                    labels = labels.cpu().data.numpy()\n",
    "                    labels = (labels==normalclass).astype(int)\n",
    "\n",
    "                    inputs = inputs.to(device)\n",
    "                    outputs = net(inputs)\n",
    "                    scores = outputs.data.cpu().numpy()\n",
    "                    idx_label_score += list(zip(labels.tolist(),\n",
    "                                                scores.tolist()))\n",
    "\n",
    "            tstlabels, scores = zip(*idx_label_score)\n",
    "            tstlabels = np.array(tstlabels)\n",
    "\n",
    "            tstlabels[np.where(tstlabels==0)]=-1.0\n",
    "            scores = np.array(scores)\n",
    "\n",
    "            Tst_auc_score = evalmetrics(tstlabels,scores.flatten())\n",
    "\n",
    "            # log epoch statistics\n",
    "            epoch_train_time = time.time() - epoch_start_time\n",
    "            logging.info('  Epoch {}/{}\\t Time: {:.3f}\\t Loss: {:.8f} \\t Test AUC :{:.4f} '\n",
    "                        .format(epoch , EPOCH, epoch_train_time, loss_epoch / n_batches, Tst_auc_score))\n",
    "\n",
    "            print('  Epoch {}/{}\\t Time: {:.3f}\\t Loss: {:.8f} \\t Test AUC :{:.4f} '\n",
    "                        .format(epoch , EPOCH, epoch_train_time, loss_epoch / n_batches, Tst_auc_score))\n",
    "\n",
    "\n",
    "    # Final Solution\n",
    "    logging.info(' Final (TOT. EPOCHS {})::  Loss: {:.8f} \\t   AUC (Test) :{:.4f} '\n",
    "            .format(EPOCH, loss_epoch / n_batches, Tst_auc_score))\n",
    "\n",
    "    print(' Final (TOT. EPOCHS {})::  Loss: {:.8f} \\t   AUC (Test) :{:.4f} '\n",
    "            .format(EPOCH, loss_epoch / n_batches, Tst_auc_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
