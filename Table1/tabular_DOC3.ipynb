{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep One Class Classification using Contradictions (DOC3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import logging\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_fscore_support\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "device = torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    \n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a. Evaluation (Loss, Eval Metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function\n",
    "class HingeLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HingeLoss, self).__init__()\n",
    "    \n",
    "    def forward(self, ypred, ytrue, margin = 1.0, smooth = False):\n",
    "        ypred = ypred.squeeze()\n",
    "        if smooth:\n",
    "            loss = torch.nn.Softplus()\n",
    "            out = torch.mean(loss(margin - (ytrue * ypred)))\n",
    "        else:\n",
    "            out = torch.mean(torch.relu(margin - (ytrue * ypred)))\n",
    "        return out\n",
    "\n",
    "def f1score(y_true, scores):\n",
    "    # Based on https://github.com/microsoft/EdgeML/blob/master/pytorch/edgeml_pytorch/trainer/drocc_trainer.py, line 147\n",
    "    # Evaluation based on https://openreview.net/forum?id=BJJLHbb0-\n",
    "    thresh = np.percentile(scores, 20)\n",
    "    y_pred = np.where(scores >= thresh, 1, 0)\n",
    "    \n",
    "    prec, recall, f1_score, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average=\"binary\")\n",
    "    return f1_score\n",
    "   \n",
    "    \n",
    "def evalmetrics(y_true,scores, metric=\"f1\"):    \n",
    "    if metric == \"f1\":\n",
    "        eval_metric = f1score(y_true, scores)\n",
    "    else:\n",
    "        eval_metric = roc_auc_score(y_true, scores)\n",
    "    \n",
    "    return eval_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b. Data Set Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        return torch.from_numpy(self.data[idx]), (self.labels[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    train_data = np.load(os.path.join(path, 'train_data.npy'), allow_pickle = True)\n",
    "    train_lab = np.ones((train_data.shape[0])) #All positive labelled data points collected\n",
    "    test_data = np.load(os.path.join(path, 'test_data.npy'), allow_pickle = True)\n",
    "    test_lab = np.load(os.path.join(path, 'test_labels.npy'), allow_pickle = True)\n",
    "    \n",
    "    num_features = train_data.shape[1]\n",
    "    \n",
    "    scaler = MinMaxScaler(feature_range=(-1., 1.))\n",
    "    scaler.fit(train_data)\n",
    "    train_data = scaler.transform(train_data)\n",
    "    test_data = scaler.transform(test_data)\n",
    "\n",
    "    train_samples = train_data.shape[0]\n",
    "    test_samples = test_data.shape[0]\n",
    "    print(\"Train Samples: \", train_samples)\n",
    "    print(\"Test Samples: \", test_samples)\n",
    "    \n",
    "    return CustomDataset(train_data, train_lab), CustomDataset(test_data, test_lab), num_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-layer perceptron with single hidden layer.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 input_dim=2,\n",
    "                 num_classes=1, \n",
    "                 num_hidden_nodes=128):\n",
    "        super(MLP, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.num_hidden_nodes = num_hidden_nodes\n",
    "        activ = nn.ReLU(True)\n",
    "        self.feature_extractor = nn.Sequential(OrderedDict([\n",
    "            ('fc', nn.Linear(self.input_dim, self.num_hidden_nodes)),\n",
    "            ('relu1', activ)]))\n",
    "        self.size_final = self.num_hidden_nodes\n",
    "\n",
    "        self.classifier = nn.Sequential(OrderedDict([\n",
    "            ('fc1', nn.Linear(self.size_final, self.num_classes))]))\n",
    "\n",
    "    def forward(self, input):\n",
    "        features = self.feature_extractor(input)\n",
    "        logits = self.classifier(features.view(-1, self.size_final))\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename='tabular_DOC3.log', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of repetitions per experiment\n",
    "# Results reported in paper are the summary statistics over 10 repetitions (i.e., n_reps = 10)\n",
    "n_reps = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0 Training data: Arrhythmia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************\n",
      "Data: Arrhythmia\n",
      "================================\n",
      "lam = 0.01, lr = 0.001, Cu = 0.001\n",
      "Exp No: 0\n",
      "Train Samples:  320\n",
      "Test Samples:  132\n",
      "  Epoch 0/300\t Time: 0.269\t Loss: 1.36924717 \t Test metric :0.7018 \n",
      "  Epoch 50/300\t Time: 0.016\t Loss: 0.38464647 \t Test metric :0.7368 \n",
      "  Epoch 100/300\t Time: 0.019\t Loss: 0.18516077 \t Test metric :0.7368 \n",
      "  Epoch 150/300\t Time: 0.017\t Loss: 0.07043993 \t Test metric :0.7368 \n",
      "  Epoch 200/300\t Time: 0.019\t Loss: 0.02529830 \t Test metric :0.7368 \n",
      "  Epoch 250/300\t Time: 0.018\t Loss: 0.00927272 \t Test metric :0.7368 \n",
      "  Epoch 300/300\t Time: 0.017\t Loss: 0.00465991 \t Test metric :0.7368 \n",
      " Final (TOT. EPOCHS 300)::  Loss: 0.00465991 \t   Metric (Test) :0.7368 \n"
     ]
    }
   ],
   "source": [
    "EPOCH = 300 \n",
    "BATCH_SIZEU = 100\n",
    "BATCH_SIZE = 100\n",
    "METRIC = 'f1'\n",
    "\n",
    "print('********************************')\n",
    "print('Data: Arrhythmia')\n",
    "logging.info('********************************')\n",
    "logging.info('Data: Arrhythmia')\n",
    "for exp in range(n_reps):\n",
    "    %run process_odds.py -d data/Arrhythmia/arrhythmia -o data/Arrhythmia/\n",
    "    for lam in [0.01]:\n",
    "        for lr in [1e-3]:\n",
    "            for Cu in [0.001]: \n",
    "\n",
    "                eps = 0.0\n",
    "                dpos = 1.0-eps\n",
    "                dneg = -(1.0+eps)\n",
    "\n",
    "                print('================================')\n",
    "                print('lam = {}, lr = {}, Cu = {}'.format(lam,lr,Cu))\n",
    "                logging.info('================================')\n",
    "                logging.info('lam = {}, lr = {}, Cu = {}'.format(lam,lr,Cu))\n",
    "\n",
    "\n",
    "\n",
    "                print('Exp No: {}'.format(exp))\n",
    "                logging.info('Exp No: {}'.format(exp))\n",
    "\n",
    "                ###############################################################################################\n",
    "                #                                     Train Data\n",
    "                ###############################################################################################\n",
    "\n",
    "                train_dataset, test_dataset, num_features = load_data(\"data/Arrhythmia\")\n",
    "                train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "\n",
    "                ###############################################################################################\n",
    "                #                                     Evaluations on :-\n",
    "                ###############################################################################################\n",
    "                # Test Data.  Arrhythmia dataset has 132 test points\n",
    "                test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=132, shuffle=False)\n",
    "\n",
    "                # Train (used to Evaluate i.e. TP_rate etc.)\n",
    "                train_loader_eval = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "\n",
    "                net = MLP(input_dim=num_features)\n",
    "                net = net.to(device)\n",
    "\n",
    "                optimizer = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.99)\n",
    "    \n",
    "                train_loss = HingeLoss()\n",
    "                unlabeled_posloss = HingeLoss()\n",
    "                unlabeled_negloss = HingeLoss()\n",
    "\n",
    "                for epoch in range(EPOCH+1):\n",
    "\n",
    "                    loss_epoch = 0.0\n",
    "                    n_batches = 0\n",
    "                    epoch_start_time = time.time()\n",
    "\n",
    "                    for data in train_loader:\n",
    "\n",
    "                        # Training Data\n",
    "                        inputs, labels = data\n",
    "                        inputs = inputs.to(device).float()\n",
    "                        ytr = np.ones((len(inputs),1))\n",
    "                        ytr = torch.from_numpy(ytr).to(device).float()\n",
    "                        outputs = net(inputs) \n",
    "\n",
    "                        # Random noise Universum\n",
    "                        XU = torch.from_numpy(np.random.uniform(low=-1., high=1.0, size=(BATCH_SIZEU, num_features)))\n",
    "\n",
    "                        XU = XU.to(device).float()\n",
    "                        outputsU = net(XU)\n",
    "                        yunppos = np.ones(BATCH_SIZEU)\n",
    "                        yunpneg = -np.ones(BATCH_SIZEU)\n",
    "                        yupos = torch.from_numpy(yunppos).to(device).float()\n",
    "                        yuneg = torch.from_numpy(yunpneg).to(device).float()\n",
    "\n",
    "                        # Zero the network parameter gradients\n",
    "                        optimizer.zero_grad()\n",
    "\n",
    "                        # Loss\n",
    "                        loss = train_loss(outputs,ytr,smooth = False)\n",
    "\n",
    "                        # Unlabeled data\n",
    "                        lossunlab = unlabeled_posloss(outputsU,yupos,dpos,smooth = False) + unlabeled_negloss(outputsU,yuneg,dneg,smooth = False)\n",
    "                        loss+=Cu*lossunlab\n",
    "\n",
    "                        lam = torch.tensor(lam).to(device)\n",
    "                        l2_reg = torch.tensor(0.).to(device)\n",
    "\n",
    "                        for name, param in net.named_parameters():\n",
    "                            l2_reg += torch.norm(param)**2\n",
    "\n",
    "                        loss += lam * l2_reg\n",
    "\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                        loss_epoch += loss.item()\n",
    "                        n_batches += 1\n",
    "\n",
    "\n",
    "                    if epoch%50==0:\n",
    "\n",
    "                        idx_label_score = []\n",
    "                        net.eval()\n",
    "                        with torch.no_grad():\n",
    "                            for data in test_loader:\n",
    "                                inputs, labels = data\n",
    "\n",
    "                                labels = labels.cpu().data.numpy()\n",
    "\n",
    "                                inputs = inputs.to(device).float()\n",
    "                                outputs = net(inputs)\n",
    "                                scores = outputs.data.cpu().numpy()\n",
    "                                idx_label_score += list(zip(labels.tolist(),\n",
    "                                                            scores.tolist()))\n",
    "\n",
    "                        tstlabels, scores = zip(*idx_label_score)\n",
    "                        tstlabels = np.array(tstlabels)\n",
    "\n",
    "                        scores = np.array(scores)\n",
    "\n",
    "                        Tst_score = evalmetrics(tstlabels,scores.flatten(), metric=METRIC)\n",
    "\n",
    "                        # log epoch statistics\n",
    "                        epoch_train_time = time.time() - epoch_start_time\n",
    "                        logging.info('  Epoch {}/{}\\t Time: {:.3f}\\t Loss: {:.8f} \\t Test metric :{:.4f} '\n",
    "                                    .format(epoch , EPOCH, epoch_train_time, loss_epoch / n_batches, Tst_score))\n",
    "\n",
    "                        print('  Epoch {}/{}\\t Time: {:.3f}\\t Loss: {:.8f} \\t Test metric :{:.4f} '\n",
    "                                    .format(epoch , EPOCH, epoch_train_time, loss_epoch / n_batches, Tst_score))\n",
    "\n",
    "\n",
    "                # Final Solution\n",
    "                logging.info(' Final (TOT. EPOCHS {})::  Loss: {:.8f} \\t   Metric (Test) :{:.4f} '\n",
    "                        .format(EPOCH, loss_epoch / n_batches, Tst_score))\n",
    "\n",
    "                print(' Final (TOT. EPOCHS {})::  Loss: {:.8f} \\t   Metric (Test) :{:.4f} '\n",
    "                        .format(EPOCH, loss_epoch / n_batches, Tst_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Training data: Thyroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************\n",
      "Data: Thyroid\n",
      "================================\n",
      "lam = 1e-06, lr = 0.001, Cu = 5.0\n",
      "Exp No: 0\n",
      "Train Samples:  6132\n",
      "Test Samples:  1068\n",
      "  Epoch 0/500\t Time: 0.211\t Loss: 3.10241471 \t Test metric :0.6484 \n",
      "  Epoch 50/500\t Time: 0.213\t Loss: 0.14915057 \t Test metric :0.7334 \n",
      "  Epoch 100/500\t Time: 0.215\t Loss: 0.11147142 \t Test metric :0.7378 \n",
      "  Epoch 150/500\t Time: 0.212\t Loss: 0.06792299 \t Test metric :0.7507 \n",
      "  Epoch 200/500\t Time: 0.208\t Loss: 0.09949921 \t Test metric :0.7450 \n",
      "  Epoch 250/500\t Time: 0.215\t Loss: 0.10656645 \t Test metric :0.7464 \n",
      "  Epoch 300/500\t Time: 0.211\t Loss: 0.05981821 \t Test metric :0.7550 \n",
      "  Epoch 350/500\t Time: 0.207\t Loss: 0.03766230 \t Test metric :0.7550 \n",
      "  Epoch 400/500\t Time: 0.203\t Loss: 0.15165700 \t Test metric :0.7579 \n",
      "  Epoch 450/500\t Time: 0.208\t Loss: 0.21051489 \t Test metric :0.7579 \n",
      "  Epoch 500/500\t Time: 0.205\t Loss: 0.10969081 \t Test metric :0.7579 \n",
      " Final (TOT. EPOCHS 500)::  Loss: 0.10969081 \t   Metric (Test) :0.7579 \n"
     ]
    }
   ],
   "source": [
    "EPOCH = 500 \n",
    "BATCH_SIZEU = 100\n",
    "BATCH_SIZE = 100\n",
    "METRIC = 'f1'\n",
    "\n",
    "print('********************************')\n",
    "print('Data: Thyroid')\n",
    "logging.info('********************************')\n",
    "logging.info('Data: Thyroid')\n",
    "for exp in range(n_reps):\n",
    "    %run process_odds.py -d data/Thyroid/annthyroid -o data/Thyroid/\n",
    "    for lam in [1e-6,]:\n",
    "        for lr in [1e-3]:\n",
    "            for Cu in [5.]:\n",
    "\n",
    "                eps = 0.0\n",
    "                dpos = 1.0-eps\n",
    "                dneg = -(1.0+eps)\n",
    "\n",
    "                print('================================')\n",
    "                print('lam = {}, lr = {}, Cu = {}'.format(lam,lr,Cu))\n",
    "                logging.info('================================')\n",
    "                logging.info('lam = {}, lr = {}, Cu = {}'.format(lam,lr,Cu))\n",
    "\n",
    "\n",
    "\n",
    "                print('Exp No: {}'.format(exp))\n",
    "                logging.info('Exp No: {}'.format(exp))\n",
    "\n",
    "                ###############################################################################################\n",
    "                #                                     Train Data\n",
    "                ###############################################################################################\n",
    "\n",
    "                train_dataset, test_dataset, num_features = load_data(\"data/Thyroid\")\n",
    "                train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "\n",
    "                ###############################################################################################\n",
    "                #                                     Evaluations on :-\n",
    "                ###############################################################################################\n",
    "                # Test Data. Thyroid dataset has 1068 test points\n",
    "                test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=1068, shuffle=False)\n",
    "\n",
    "                # Train (used to Evaluate i.e. TP_rate etc.)\n",
    "                train_loader_eval = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "\n",
    "                net = MLP(input_dim=num_features)\n",
    "                net = net.to(device)\n",
    "\n",
    "                optimizer = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.99)  \n",
    "\n",
    "                train_loss = HingeLoss()\n",
    "                unlabeled_posloss = HingeLoss()\n",
    "                unlabeled_negloss = HingeLoss()\n",
    "\n",
    "                for epoch in range(EPOCH+1):\n",
    "\n",
    "                    loss_epoch = 0.0\n",
    "                    n_batches = 0\n",
    "                    epoch_start_time = time.time()\n",
    "\n",
    "                    for data in train_loader:\n",
    "\n",
    "                        # Training Data\n",
    "                        inputs, labels = data\n",
    "                        inputs = inputs.to(device).float()\n",
    "                        ytr = np.ones((len(inputs),1))\n",
    "                        ytr = torch.from_numpy(ytr).to(device).float()\n",
    "                        outputs = net(inputs) \n",
    "\n",
    "                        # Random noise Universum\n",
    "                        XU = torch.from_numpy(np.random.uniform(low=-1., high=1.0, size=(BATCH_SIZEU, num_features)))\n",
    "\n",
    "                        XU = XU.to(device).float()\n",
    "                        outputsU = net(XU)\n",
    "                        yunppos = np.ones(BATCH_SIZE)\n",
    "                        yunpneg = -np.ones(BATCH_SIZE)\n",
    "                        yupos = torch.from_numpy(yunppos).to(device).float()\n",
    "                        yuneg = torch.from_numpy(yunpneg).to(device).float()\n",
    "\n",
    "\n",
    "                        # Zero the network parameter gradients\n",
    "                        optimizer.zero_grad()\n",
    "\n",
    "                        # Loss\n",
    "                        loss = train_loss(outputs,ytr,smooth = False)\n",
    "\n",
    "                        # Unlabeled data\n",
    "                        lossunlab = unlabeled_posloss(outputsU,yupos,dpos,smooth = False) + unlabeled_negloss(outputsU,yuneg,dneg,smooth = False)\n",
    "                        loss+=Cu*lossunlab\n",
    "\n",
    "                        lam = torch.tensor(lam).to(device)\n",
    "                        l2_reg = torch.tensor(0.).to(device)\n",
    "\n",
    "                        for name, param in net.named_parameters():\n",
    "                            l2_reg += torch.norm(param)**2\n",
    "\n",
    "                        loss += lam * l2_reg\n",
    "\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                        loss_epoch += loss.item()\n",
    "                        n_batches += 1\n",
    "\n",
    "\n",
    "                    if epoch%50==0:\n",
    "\n",
    "                        idx_label_score = []\n",
    "                        net.eval()\n",
    "                        with torch.no_grad():\n",
    "                            for data in test_loader:\n",
    "                                inputs, labels = data\n",
    "\n",
    "                                labels = labels.cpu().data.numpy()\n",
    "\n",
    "                                inputs = inputs.to(device).float()\n",
    "                                outputs = net(inputs)\n",
    "                                scores = outputs.data.cpu().numpy()\n",
    "                                idx_label_score += list(zip(labels.tolist(),\n",
    "                                                            scores.tolist()))\n",
    "\n",
    "                        tstlabels, scores = zip(*idx_label_score)\n",
    "                        tstlabels = np.array(tstlabels)\n",
    "\n",
    "                        scores = np.array(scores)\n",
    "                        \n",
    "                        Tst_score = evalmetrics(tstlabels,scores.flatten(), metric=METRIC)\n",
    "\n",
    "                        # log epoch statistics\n",
    "                        epoch_train_time = time.time() - epoch_start_time\n",
    "                        logging.info('  Epoch {}/{}\\t Time: {:.3f}\\t Loss: {:.8f} \\t Test metric :{:.4f} '\n",
    "                                    .format(epoch , EPOCH, epoch_train_time, loss_epoch / n_batches, Tst_score))\n",
    "\n",
    "                        print('  Epoch {}/{}\\t Time: {:.3f}\\t Loss: {:.8f} \\t Test metric :{:.4f} '\n",
    "                                    .format(epoch , EPOCH, epoch_train_time, loss_epoch / n_batches, Tst_score))\n",
    "\n",
    "\n",
    "                # Final Solution\n",
    "                logging.info(' Final (TOT. EPOCHS {})::  Loss: {:.8f} \\t   Metric (Test) :{:.4f} '\n",
    "                        .format(EPOCH, loss_epoch / n_batches, Tst_score))\n",
    "\n",
    "                print(' Final (TOT. EPOCHS {})::  Loss: {:.8f} \\t   Metric (Test) :{:.4f} '\n",
    "                        .format(EPOCH, loss_epoch / n_batches, Tst_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Training data: Abalone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************\n",
      "Data: Abalone\n",
      "================================\n",
      "lam = 0.1, lr = 0.001, Cu = 0.01\n",
      "Exp No: 0\n",
      "Train Samples:  1862\n",
      "Test Samples:  58\n",
      "  Epoch 0/300\t Time: 0.082\t Loss: 5.04095170 \t Test metric :0.7200 \n",
      "  Epoch 50/300\t Time: 0.071\t Loss: 0.09643767 \t Test metric :0.7733 \n",
      "  Epoch 100/300\t Time: 0.073\t Loss: 0.09609424 \t Test metric :0.7733 \n",
      "  Epoch 150/300\t Time: 0.067\t Loss: 0.09611613 \t Test metric :0.7733 \n",
      "  Epoch 200/300\t Time: 0.070\t Loss: 0.09612126 \t Test metric :0.7733 \n",
      "  Epoch 250/300\t Time: 0.070\t Loss: 0.09610527 \t Test metric :0.7733 \n",
      "  Epoch 300/300\t Time: 0.070\t Loss: 0.09614519 \t Test metric :0.7733 \n",
      " Final (TOT. EPOCHS 300)::  Loss: 0.09614519 \t   Metric (Test) :0.7733 \n"
     ]
    }
   ],
   "source": [
    "EPOCH = 300 \n",
    "BATCH_SIZEU = 100\n",
    "BATCH_SIZE = 100\n",
    "METRIC = 'f1'\n",
    "\n",
    "print('********************************')\n",
    "print('Data: Abalone')\n",
    "logging.info('********************************')\n",
    "logging.info('Data: Abalone')\n",
    "for exp in range(n_reps):\n",
    "    %run process_abalone.py -d data/Abalone/abalone.data -o data/Abalone/\n",
    "    for lam in [0.1]:\n",
    "        for lr in [1e-3]:\n",
    "            for Cu in [0.01]:\n",
    "\n",
    "                eps = 0.0\n",
    "                dpos = 1.0-eps\n",
    "                dneg = -(1.0+eps)\n",
    "\n",
    "                print('================================')\n",
    "                print('lam = {}, lr = {}, Cu = {}'.format(lam,lr,Cu))\n",
    "                logging.info('================================')\n",
    "                logging.info('lam = {}, lr = {}, Cu = {}'.format(lam,lr,Cu))\n",
    "\n",
    "\n",
    "                print('Exp No: {}'.format(exp))\n",
    "                logging.info('Exp No: {}'.format(exp))\n",
    "\n",
    "                ###############################################################################################\n",
    "                #                                     Train Data\n",
    "                ###############################################################################################\n",
    "\n",
    "                train_dataset, test_dataset, num_features = load_data(\"data/Abalone\")\n",
    "                train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "\n",
    "                ###############################################################################################\n",
    "                #                                     Evaluations on :-\n",
    "                ###############################################################################################\n",
    "                # Test Data. Abalone dataset has 58 test points\n",
    "                test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=58, shuffle=False)\n",
    "\n",
    "                # Train (used to Evaluate i.e. TP_rate etc.)\n",
    "                train_loader_eval = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "\n",
    "                net = MLP(input_dim=num_features)\n",
    "                net = net.to(device)\n",
    "\n",
    "                optimizer = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.99)  \n",
    "\n",
    "                train_loss = HingeLoss()\n",
    "                unlabeled_posloss = HingeLoss()\n",
    "                unlabeled_negloss = HingeLoss()\n",
    "\n",
    "                for epoch in range(EPOCH+1):\n",
    "\n",
    "                    loss_epoch = 0.0\n",
    "                    n_batches = 0\n",
    "                    epoch_start_time = time.time()\n",
    "\n",
    "                    for data in train_loader:\n",
    "\n",
    "                        # Training Data\n",
    "                        inputs, labels = data\n",
    "                        inputs = inputs.to(device).float()\n",
    "                        ytr = np.ones((len(inputs),1))\n",
    "                        ytr = torch.from_numpy(ytr).to(device).float()\n",
    "                        outputs = net(inputs) \n",
    "\n",
    "                        # Random noise Universum\n",
    "                        XU = torch.from_numpy(np.random.uniform(low=-1., high=1.0, size=(BATCH_SIZEU, num_features)))\n",
    "        \n",
    "                        XU = XU.to(device).float()\n",
    "                        outputsU = net(XU)\n",
    "                        yunppos = np.ones(BATCH_SIZE)\n",
    "                        yunpneg = -np.ones(BATCH_SIZE)\n",
    "                        yupos = torch.from_numpy(yunppos).to(device).float()\n",
    "                        yuneg = torch.from_numpy(yunpneg).to(device).float()\n",
    "\n",
    "\n",
    "                        # Zero the network parameter gradients\n",
    "                        optimizer.zero_grad()\n",
    "\n",
    "                        # Loss\n",
    "                        loss = train_loss(outputs,ytr,smooth = False)\n",
    "\n",
    "                        # Unlabeled data\n",
    "                        lossunlab = unlabeled_posloss(outputsU,yupos,dpos,smooth = False) + unlabeled_negloss(outputsU,yuneg,dneg,smooth = False)\n",
    "                        loss+=Cu*lossunlab\n",
    "\n",
    "                        lam = torch.tensor(lam).to(device)\n",
    "                        l2_reg = torch.tensor(0.).to(device)\n",
    "\n",
    "                        for name, param in net.named_parameters():\n",
    "                            l2_reg += torch.norm(param)**2\n",
    "\n",
    "                        loss += lam * l2_reg\n",
    "\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                        loss_epoch += loss.item()\n",
    "                        n_batches += 1\n",
    "\n",
    "\n",
    "                    if epoch%50==0:\n",
    "\n",
    "                        idx_label_score = []\n",
    "                        net.eval()\n",
    "                        with torch.no_grad():\n",
    "                            for data in test_loader:\n",
    "                                inputs, labels = data\n",
    "\n",
    "                                labels = labels.cpu().data.numpy()\n",
    "\n",
    "                                inputs = inputs.to(device).float()\n",
    "                                outputs = net(inputs)\n",
    "                                scores = outputs.data.cpu().numpy()\n",
    "                                idx_label_score += list(zip(labels.tolist(),\n",
    "                                                            scores.tolist()))\n",
    "\n",
    "                        tstlabels, scores = zip(*idx_label_score)\n",
    "                        tstlabels = np.array(tstlabels)\n",
    "\n",
    "                        scores = np.array(scores)\n",
    "\n",
    "                        Tst_score = evalmetrics(tstlabels,scores.flatten(), metric=METRIC)\n",
    "\n",
    "                        # log epoch statistics\n",
    "                        epoch_train_time = time.time() - epoch_start_time\n",
    "                        logging.info('  Epoch {}/{}\\t Time: {:.3f}\\t Loss: {:.8f} \\t Test metric :{:.4f} '\n",
    "                                    .format(epoch , EPOCH, epoch_train_time, loss_epoch / n_batches, Tst_score))\n",
    "\n",
    "                        print('  Epoch {}/{}\\t Time: {:.3f}\\t Loss: {:.8f} \\t Test metric :{:.4f} '\n",
    "                                    .format(epoch , EPOCH, epoch_train_time, loss_epoch / n_batches, Tst_score))\n",
    "\n",
    "\n",
    "                # Final Solution\n",
    "                logging.info(' Final (TOT. EPOCHS {})::  Loss: {:.8f} \\t   Metric (Test) :{:.4f} '\n",
    "                        .format(EPOCH, loss_epoch / n_batches, Tst_score))\n",
    "\n",
    "                print(' Final (TOT. EPOCHS {})::  Loss: {:.8f} \\t   Metric (Test) :{:.4f} '\n",
    "                        .format(EPOCH, loss_epoch / n_batches, Tst_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
